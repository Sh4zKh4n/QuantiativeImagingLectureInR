---
title       : Post Segementation Quiz
subtitle    : Shape Analysis to Dynamic Experiments
author      : Kevin Mader
job         : Lecturer, ETH Zurich 
license     : by-nc-sa
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : solarized_light      # 
widgets     : [mathjax,quiz,bootstrap] #[mathjax, bootstrap, quiz, opencpu, popcornjs]
mode        : selfcontained # {standalone, draft}
github:
  user: kmader
  repo: Quantitative-Big-Imaging-Course
---
```{r global_setup,  warning=FALSE, cache=FALSE,echo=FALSE,error=FALSE,results='hide'}
require(knitr)
# default settings, # settings for presentation version
echo.val<-F
fig.height<-5
dpi<-150
cache<-T
fig.path<-"pres_figures/"
cache.path<-"pres_cache/"

if(exists("printed")) { # settings for printed version (if the variable exists)
  echo.val<-T # show code
  fig.height<-3
  dpi<-150
  cache<-T
  fig.path<-"print_figures/"
  cache.path<-"print_cache/"
}


opts_chunk$set(dpi=dpi,cache=cache,
               cache.path=cache.path,results='hide',
               warning=F,fig.align='center',echo=echo.val,
               fig.path=fig.path,message=F) #dev="CairoPNG"

knit_hooks$set(source = function(x, options){
  if (!is.null(options$knitCode) && (options$knitCode)){
    paste("<div><textarea class='knitCode' style='display:none;'>", x, 
      "</textarea></div>", sep = '\n')
  } else {
    stringr::str_c("\n\n```", tolower(options$engine), "\n", x, "```\n\n")
  }
})
knit_hooks$set(document = function(x){
  gsub("```\n*```r*\n*", "", x)
})
```

```{r script_setup,results='hide',cache=FALSE}
require(ggplot2)
require(plyr)
require(grid) # contains the arrow function
require(biOps)
require(doMC) # for parallel code
require(EBImage)
## To install EBImage
# source("http://bioconductor.org/biocLite.R")
# biocLite("EBImage")

# start parallel environment
registerDoMC()
# functions for converting images back and forth
im.to.df<-function(in.img) {
    out.im<-expand.grid(x=1:nrow(in.img),y=1:ncol(in.img))
    out.im$val<-as.vector(in.img)
    out.im
}
df.to.im<-function(in.df,val.col="val",inv=F) {
  in.vals<-in.df[[val.col]]
  if(class(in.vals[1])=="logical") in.vals<-as.integer(in.vals*255)
  if(inv) in.vals<-255-in.vals
  out.mat<-matrix(in.vals,nrow=length(unique(in.df$x)),byrow=F)
  attr(out.mat,"type")<-"grey"
  out.mat
}
ddply.cutcols<-function(...,cols=1) {
  # run standard ddply command
  cur.table<-ddply(...)
  cutlabel.fixer<-function(oVal) {
    sapply(oVal,function(x) {
      cnv<-as.character(x)
      mean(as.numeric(strsplit(substr(cnv,2,nchar(cnv)-1),",")[[1]]))
    })
  }
  cutname.fixer<-function(c.str) {
    s.str<-strsplit(c.str,"(",fixed=T)[[1]]
    t.str<-strsplit(paste(s.str[c(2:length(s.str))],collapse="("),",")[[1]]
    paste(t.str[c(1:length(t.str)-1)],collapse=",")
  }
  for(i in c(1:cols)) {
    cur.table[,i]<-cutlabel.fixer(cur.table[,i])
    names(cur.table)[i]<-cutname.fixer(names(cur.table)[i])
  }
  cur.table
}
```

## Enhancement and Segmentation Quiz

The quiz is not graded but it will help you identify which areas to better review. It is also good preparation for the next section since the course builds on itself the better you understand the first lectures the easier the subsequent ones will be.


--- &radio2
## Choosing a threshold

Based just on the distribution above which value would make the most sense for the threshold

1. 0.5
1. _1.0_
1. 1.75

*** =image
```{r, fig.cap=""}
nx<-10
ny<-10
cross.im<-expand.grid(x=c(-nx:nx)/nx*2*pi,y=c(-ny:ny)/ny*2*pi)
cross.im<-cbind(cross.im,
               col=1.5*with(cross.im,abs(cos(x*y))/(abs(x*y)+(3*pi/nx)))+
                 0.5*runif(nrow(cross.im)))
bn.wid<-diff(range(cross.im$col))/20
ggplot(cross.im,aes(x=col))+geom_histogram(binwidth=bn.wid)+
  labs(x="Intensity",title="Probability Density Function")+
  theme_bw(20)
```

*** .explanation
The value 1.0 lies best between the two peaks of the distribution


--- &radio2
## Morphological Operations

Looking at the before and after images, which morphological operation was most-likely performed and what was the range

1. Erosion, 3x3
1. Dilation, 3x3
1. Opening, 3x3
1. Closing, 3x3
1. Erosion, 11x11
1. Dilation, 11x11
1. Opening, 11x11
1. _Closing, 11x11_

*** =image
```{r, fig.cap="Cortical Segment Morphology"}

```

*** .hint
Pay attention to the edges of the image and the size of the features inside
*** .explanation
### Kernel Shape
The operation can be seen by looking at the inside and edges. Since the inside is filled in, it must be either dilation or closing (the other operations typically only remove pixels). By examining the edges and seeing that the structure does not grow we can determine it is closing (dilation would have caused the borders to grow).
### Neighborhood
The size or neighborhood can be guessed by looking at the size of the holes filled which are clearly much larger than 3 pixels.

--- &submitcompare2 rows:5
## Morphological Kernels / Neighborhoods

Looking at the before and after images, which neighborhood or kernel was used with the closing operation

```{r, fig.cap="Cortical Segment Closing"}

```

*** .hint
The possible kernel choices are box, diamond, line, and disc (circular)
*** .explanation
The images were dilated with the disc (circular), box (full), diamond, and line neighborhoods respectively. This can be seen by looking at the borders of the shape which are very boxy in 2 & 4 and very curved in 1. In 3 they are jagged indicative of a diamond shape. 2 is box because it extends in both x and y while 4 only extends along the y direction.

--- &submitcompare2 rows:5
## Segmentation Strategy

You measure the same cross 4 times and and get the following images as output. Estimate the signal to noise in the images and provide a strategy for how the data could be filtered and segmented.

```{r, fig.cap="",fig.height=5}
nx<-10
ny<-10
cross.im<-expand.grid(x=c(-nx:nx)/nx*2*pi,y=c(-ny:ny)/ny*2*pi)
noise.levels<-c(0,0.5,3,0.25)
cross.im.fn<-function(noise.idx) cbind(cross.im,
               col=1.5*with(cross.im,abs(cos(x*y))/(abs(x*y)+(3*pi/nx)))+
                 noise.levels[noise.idx]*runif(nrow(cross.im)),
               noise.level=noise.levels[noise.idx],
               noise.idx=noise.idx)

ggplot(ldply(1:length(noise.levels),cross.im.fn),aes(x=x,y=y,fill=col))+
  geom_raster()+facet_wrap(~noise.idx)+
  labs(fill="Intensity",title="Input Images")+
  scale_fill_gradientn(colours=c("red","green","blue"))+
  theme_bw(20)#+guides()
```

*** .explanation
The signal to noise ratios for the image are very high (>3) in 1 and 4,  reasonable in 2, and completely unusable in 4. The images in 1,2 and 4 can be processed using a Gaussian filter and threshold while might have a tiny bit of information but would need to be heavily filtered and interpreted carefully. It is easiest therefore, to examine the histograms of the images first where 3 has a much broader peak (higher standard deviation) indicating it is noisier and skipping this image in the first place.

*** .hint
See Enhancement and Segmentation, you do not need to keep all of the data.

--- &radio
## Automatic Thresholds

Which is the best reason to use automatic threshold techniques on data

1. Reduce noise
2. _Compensate for changing illumination_
3. Improve signal to noise ratio
4. Segment difficult to separate phases

*** .hint
Check the "Where segmentation fails" slide

*** .explanation
Changing illumination will change the brightness of the pixels in the image but should not largely change their statistics or distribution making them ideal candidates for automated methods.
```{r, fig.cap="Varying Illumination Images"}
cellImage<-im.to.df(jpeg::readJPEG("../ext-figures/Cell_Colony.jpg"))
il.vals<-runif(9,min=0.2,max=1/0.2)
im.vals<-ldply(1:length(il.vals),function(il.idx,th.val=0.75)
  cbind(cellImage[,c("x","y")],
        val=cellImage$val*il.vals[il.idx],
        in.thresh=ifelse(cellImage$val*il.vals[il.idx]<th.val,"Cells","Background"),
        il.val=il.vals[il.idx],
        th.val=th.val
        ))
im.vals$il.val<-as.numeric(factor(im.vals$il.val))
ggplot(im.vals,aes(x=x,y=y,fill=val))+
  geom_raster()+facet_wrap(~il.val)+
  labs(fill="Intensity")+
  theme_bw(20)+coord_equal()
```
```{r, fig.cap="Varying Illumination Histograms"}
ggplot(im.vals,aes(x=val,color=as.factor(il.val)))+
  geom_density()+
  labs(x="Intensity",y="Frequency",color="Image")+
  theme_bw(20)

```
Neither noise nor signal to noise ratio can be improved using automated segmentation. These are addressed soley in the "Image Enhancement" lecture of the course.
While automatic techniques might make results more reliable in 4, difficult segmentations are just as difficult when using automated techniques


--- &radio2
## Selecting a threshold technique

Based soley on the histogram above which automatic threshold technique is best suited?

1. _Intermodes_
1. Hysteresis Threshold
1. K-Means

*** =image
```{r, fig.cap=""}
nx<-10
ny<-10
cross.im<-expand.grid(x=c(-nx:nx)/nx*2*pi,y=c(-ny:ny)/ny*2*pi)
cross.im<-cbind(cross.im,
               col=1.5*with(cross.im,abs(cos(x*y))/(abs(x*y)+(3*pi/nx)))+
                 0.5*runif(nrow(cross.im)))
bn.wid<-diff(range(cross.im$col))/20
ggplot(cross.im,aes(x=col))+geom_histogram(binwidth=bn.wid)+
  labs(x="Intensity",title="Probability Density Function")+
  theme_bw(20)
```

*** .explanation
Intermodes works best since the two phases are well characterized wiht the modes of the system and since their does not appear to be any bias or skew the method taking the value between them would work best.
### Wrong Answers
K-Means and Hysteresis might work as well but it is hard to know without seeing the image itself

*** .hint
See Advanced Segmentation Lecture


--- &submitcompare1 rows:5
## IsoData
Explain how the IsoData method works for finding a threshold. To what kind of images to you think it would apply well?

### Hint
It is not in the lecture, you need to search online a bit.

*** .explanation
IsoData works by picking a threshold $G$ and applying it to the image. The image is then divided into two groups $A$ is all points $\geq G$ and $B$ is all points $\leq G$. The average intensity value is calculated in $A$ and $B$ and a new $G = \frac{\mu_A+\mu_B}{2}$. 
The method would work well for images with varying illumination or other issues and is in many cases more robust than intermodes since it does not require a smoothing step.


--- &submitcompare2 rows:5
## Applying Thresholds

Can a threshold be applied to this image? Why or why not?
```{r, fig.cap="",fig.height=4}
cellImage<-im.to.df(jpeg::readJPEG("../ext-figures/Cell_Colony.jpg"))
cellImage$ilval<-with(cellImage,val+sqrt((x-200)^2+(y-200)^2)/100)
ggplot(cellImage,aes(x=x,y=y,fill=ilval))+
  geom_raster()+
  labs(fill="Intensity")+
  theme_bw(20)+coord_equal()
```
```{r, fig.cap="",fig.height=2}

ggplot(cellImage,aes(x=ilval))+
  geom_density()+
  labs(x="Intensity",y="Frequency")+
  theme_bw(20)
```

*** .hint
Try to divide the image into two regions cell and background

*** .explanation
A threshold cannot be applied to the image because there is significant structure in the background. The first step is to correct this structure and then apply a threshold. If this is not possible edge enhancement filters can be applied to improve the image and make the different phases easier to distinguish.

--- &radio
## K-Means Clustering

Which statement best describes the limitations and shortcomings of using K-means clustering

1. Since it is an iterative method it can take a long time to converge
1. _The random seed start locations can influence the final result and running the analysis multiple times often results in multiple different answers_
1. K-Means can only handle position information
1. K-Means can only handle small imaging datasets
1. K-Means is complicated and therefore not many programs support it.

*** .hint
Check the "K-Means" slides in the Advanced Segmentation presentation

*** .explanation
The start locations for the clusters in K-means are random and in many situations the start locations greatly influence the final groups (local-minima problem). The issue can be alleviated by running K-means many times on the same dataset but when different data is used it is always possible that the resulting classification is very different. The best way around this is to use K-means to establish a standard set of segmentation or thresholding criteria and then apply these criteria (which are now fixed and not subject to random cluster locations) to the new datasets rather than K-means itself. 
### Wrong Answers
While K-means is iterative, it has been optimized and can run very quickly even on massive datasets. K-means has been implemented on systems where handle enormous datasets (much larger than ImageJ or Matlab) and given its vector space formulation can handle nearly any kind of information associated to each point. Every major math or programming tool has support for K-means (Matlab, Java, Python, Octave, R, Paraview, even Excel with an Add-In) and it is very easy to implement.





