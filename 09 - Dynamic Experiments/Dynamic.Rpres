```{r global_setup,  warning=FALSE, cache=FALSE,echo=FALSE,error=FALSE,results='hide'}
require(knitr)
# default settings, # settings for presentation version
echo.val<-F
fig.height<-5
dpi<-80
cache<-T
fig.path<-"pres_figures/"
cache.path<-"pres_cache/"

if(exists("printed")) { # settings for printed version (if the variable exists)
  echo.val<-T # show code
  fig.height<-3
  dpi<-100
  cache<-T
  fig.path<-"print_figures/"
  cache.path<-"print_cache/"
  }

opts_chunk$set(dpi=dpi,cache=cache,
               cache.path=cache.path,results='hide',
               warning=F,fig.align='center',echo=echo.val,
               fig.height=fig.height,fig.path=fig.path,message=F,autodep=TRUE) #dev="CairoPNG"
dep_auto()
```

```{r script_setup,results='hide',cache=FALSE}
require(ggplot2)
require(lattice) # nicer scatter plots
require(plyr)
require(grid) # contains the arrow function
require(biOps)
require(doMC) # for parallel code
require(EBImage)
require(reshape2) # for the melt function
## To install EBImage
# source("http://bioconductor.org/biocLite.R")
# biocLite("EBImage")
require(png)
require(gridExtra)

# start parallel environment
registerDoMC()
# functions for converting images back and forth
im.to.df<-function(in.img) {
  out.im<-expand.grid(x=1:nrow(in.img),y=1:ncol(in.img))
  out.im$val<-as.vector(in.img)
  out.im
  }
df.to.im<-function(in.df,val.col="val",inv=F) {
  in.vals<-in.df[[val.col]]
  if(class(in.vals[1])=="logical") in.vals<-as.integer(in.vals*255)
  if(inv) in.vals<-255-in.vals
  out.mat<-matrix(in.vals,nrow=length(unique(in.df$x)),byrow=F)
  attr(out.mat,"type")<-"grey"
  out.mat
  }
ddply.cutcols<-function(...,cols=1) {
  # run standard ddply command 
  cur.table<-ddply(...)
  cutlabel.fixer<-function(oVal) {
    sapply(oVal,function(x) {
      cnv<-as.character(x)
      mean(as.numeric(strsplit(substr(cnv,2,nchar(cnv)-1),",")[[1]]))
      })
    }
  cutname.fixer<-function(c.str) {
    s.str<-strsplit(c.str,"(",fixed=T)[[1]]
    t.str<-strsplit(paste(s.str[c(2:length(s.str))],collapse="("),",")[[1]]
    paste(t.str[c(1:length(t.str)-1)],collapse=",")
    }
  for(i in c(1:cols)) {
    cur.table[,i]<-cutlabel.fixer(cur.table[,i])
    names(cur.table)[i]<-cutname.fixer(names(cur.table)[i])
    }
  cur.table
  }


show.pngs.as.grid<-function(file.list,title.fun,zoom=1) {
  preparePng<-function(x) rasterGrob(readPNG(x,native=T,info=T),width=unit(zoom,"npc"),interp=F)
  labelPng<-function(x,title="junk") (qplot(1:300, 1:300, geom="blank",xlab=NULL,ylab=NULL,asp=1)+
                                        annotation_custom(preparePng(x))+
                                        labs(title=title)+theme_bw(24)+
                                        theme(axis.text.x = element_blank(),
                                             axis.text.y = element_blank()))
  imgList<-llply(file.list,function(x) labelPng(x,title.fun(x)) )
  do.call(grid.arrange,imgList)
}

```

```{r utility_functions,results='hide',cache=FALSE}
## Standard image processing tools which I use for visualizing the examples in the script
commean.fun<-function(in.df) {
  ddply(in.df,.(val), function(c.cell) {
    weight.sum<-sum(c.cell$weight)
    data.frame(xv=mean(c.cell$x),
               yv=mean(c.cell$y),
               xm=with(c.cell,sum(x*weight)/weight.sum),
               ym=with(c.cell,sum(y*weight)/weight.sum)
               )
    })
  }

colMeans.df<-function(x,...) as.data.frame(t(colMeans(x,...)))

pca.fun<-function(in.df) {
  ddply(in.df,.(val), function(c.cell) {
    c.cell.cov<-cov(c.cell[,c("x","y")])
    c.cell.eigen<-eigen(c.cell.cov)
    
    c.cell.mean<-colMeans.df(c.cell[,c("x","y")])
    out.df<-cbind(c.cell.mean,
                  data.frame(vx=c.cell.eigen$vectors[1,],
                             vy=c.cell.eigen$vectors[2,],
                             vw=sqrt(c.cell.eigen$values),
                             th.off=atan2(c.cell.eigen$vectors[2,],c.cell.eigen$vectors[1,]))
                  )
    })
  }
vec.to.ellipse<-function(pca.df) {
  ddply(pca.df,.(val),function(cur.pca) {
    # assume there are two vectors now
    create.ellipse.points(x.off=cur.pca[1,"x"],y.off=cur.pca[1,"y"],
                          b=sqrt(5)*cur.pca[1,"vw"],a=sqrt(5)*cur.pca[2,"vw"],
                          th.off=pi/2-atan2(cur.pca[1,"vy"],cur.pca[1,"vx"]),
                          x.cent=cur.pca[1,"x"],y.cent=cur.pca[1,"y"])
    })
  }

# test function for ellipse generation
# ggplot(ldply(seq(-pi,pi,length.out=100),function(th) create.ellipse.points(a=1,b=2,th.off=th,th.val=th)),aes(x=x,y=y))+geom_path()+facet_wrap(~th.val)+coord_equal()
create.ellipse.points<-function(x.off=0,y.off=0,a=1,b=NULL,th.off=0,th.max=2*pi,pts=36,...) {
  if (is.null(b)) b<-a
  th<-seq(0,th.max,length.out=pts)
  data.frame(x=a*cos(th.off)*cos(th)+b*sin(th.off)*sin(th)+x.off,
             y=-1*a*sin(th.off)*cos(th)+b*cos(th.off)*sin(th)+y.off,
             id=as.factor(paste(x.off,y.off,a,b,th.off,pts,sep=":")),...)
  }
deform.ellipse.draw<-function(c.box) {
  create.ellipse.points(x.off=c.box$x[1],
                        y.off=c.box$y[1],
                        a=c.box$a[1],
                        b=c.box$b[1],
                        th.off=c.box$th[1],
                        col=c.box$col[1])                    
  }
bbox.fun<-function(in.df) {
  ddply(in.df,.(val), function(c.cell) {
    c.cell.mean<-colMeans.df(c.cell[,c("x","y")])
    xmn<-emin(c.cell$x)
    xmx<-emax(c.cell$x)
    ymn<-emin(c.cell$y)
    ymx<-emax(c.cell$y)
    out.df<-cbind(c.cell.mean,
                  data.frame(xi=c(xmn,xmn,xmx,xmx,xmn),
                             yi=c(ymn,ymx,ymx,ymn,ymn),
                             xw=xmx-xmn,
                             yw=ymx-ymn
                             ))
    })
  }

# since the edge of the pixel is 0.5 away from the middle of the pixel
emin<-function(...) min(...)-0.5
emax<-function(...) max(...)+0.5
extents.fun<-function(in.df) {
  ddply(in.df,.(val), function(c.cell) {
    c.cell.mean<-colMeans.df(c.cell[,c("x","y")])
    out.df<-cbind(c.cell.mean,data.frame(xmin=c(c.cell.mean$x,emin(c.cell$x)),
                                         xmax=c(c.cell.mean$x,emax(c.cell$x)),
                                         ymin=c(emin(c.cell$y),c.cell.mean$y),
                                         ymax=c(emax(c.cell$y),c.cell.mean$y)))
    })
  }

th_fillmap.fn<-function(max.val) scale_fill_gradientn(colours=rainbow(10),limits=c(0,max.val))
```



Quantitative Big Imaging 
========================================================
author: Kevin Mader
date: 17 April 2014
width: 1440
height: 900
css: ../template.css
transition: rotate

ETHZ: 227-0966-00L
# Dynamic Experiments

Course Outline
========================================================
- 20th February - Introductory Lecture
- 27th February - Filtering and Image Enhancement (A. Kaestner)
- 6th March - Basic Segmentation, Discrete Binary Structures
- 13th March - Advanced Segmentation
- 20th March - Analyzing Single Objects
- 27th March - Analyzing Complex Objects
- 3rd April - Many Objects and Distributions
- 10th April - Statistics and Reproducibility
- 17th April - **Dynamic Experiments**
- 8th May - Big Data
- 15th May - Guest Lecture - In-Operando Imaging of Batteries (V. Wood)
- 22th May - Project Presentations

Literature / Useful References
========================================================
### Books
- Jean Claude, Morphometry with R
 - [Online](http://link.springer.com/book/10.1007%2F978-0-387-77789-4) through ETHZ
 - [Buy it](http://www.amazon.com/Morphometrics-R-Use-Julien-Claude/dp/038777789X)
- John C. Russ, “The Image Processing Handbook”,(Boca Raton, CRC Press)
 - Available [online](http://dx.doi.org/10.1201/9780203881095) within domain ethz.ch (or proxy.ethz.ch / public VPN) 

*** 

### Papers / Sites

- Comparsion of Tracking Methods in Biology
 - Chenouard, N., Smal, I., de Chaumont, F., Maška, M., Sbalzarini, I. F., Gong, Y., … Meijering, E. (2014). Objective comparison of particle tracking methods. Nature Methods, 11(3), 281–289. doi:10.1038/nmeth.2808
 - Maska, M., Ulman, V., Svoboda, D., Matula, P., Matula, P., Ederra, C., … Ortiz-de-Solorzano, C. (2014). A benchmark for comparison of cell tracking algorithms. Bioinformatics (Oxford, England), btu080–. doi:10.1093/bioinformatics/btu080
 
- Multiple Hypothesis Testing
 - Coraluppi, S. & Carthel, C. Multi-stage multiple-hypothesis tracking.
J. Adv. Inf. Fusion 6, 57–67 (2011).
 - Chenouard, N., Bloch, I. & Olivo-Marin, J.-C. Multiple hypothesis
tracking in microscopy images. in Proc. IEEE Int. Symp. Biomed. Imaging
1346–1349 (IEEE, 2009).
 
Previously on QBI ...
========================================================

- Image Enhancment 
 - Highlighting the contrast of interest in images
 - Minimizing Noise
- Understanding image histograms
- Automatic Methods
- Component Labeling
- Single Shape Analysis
- Complicated Shapes
- Distribution Analysis

Quantitative "Big" Imaging
====

The course has covered imaging enough and there have been a few quantitative metrics, but "big" has not really entered.

What does __big__ mean?
- Not just / even large
- it means being ready for _big data_
- volume, velocity, variety (3 V's)
- scalable, fast, easy to customize

***

So what is "big" imaging
- doing analyses in a disciplined manner
 - fixed steps
 - easy to regenerate results
 - no _magic_
- having everything automated
 - 100 samples is as easy as 1 sample
- being able to adapt and reuse analyses
 - one really well working script and modify parameters
 - different types of cells
 - different regions

 
Objectives
===
1. What sort of dynamic experiments do we have?
1. How can we design good dynamic experiments?
1. How can we track objects between points?
 - How can we track shape?
 - How can we track distribution?
1. How can we track topology?
1. How can we track voxels?
1. How can we assess deformation and strain?
1. How can assess more general cases?

Outline
========================================================

- Motivation (Why and How?)
- Scientific Goals
- Experiments
 - Simulations
 - Experiment Design
- Object Tracking
- Distribution
- Topology
- Voxel-based Methods
 - Cross Correlation
 - DIC
 - DIC + Physics
- General Problems
 - Thickness - Lung Tissue
 - Curvature - Metal Systems
 - Two Point Correlation - Volcanic Rock

Motivation
===

- 3D images are already difficult to interpret on their own
- 3D movies (4D) are almost impossible 

<video controls>
  <source src="ext-figures/dk31-plat.avi" type="video/avi">
Your browser does not support the video tag.
</video>

***

- 2D movies (3D) can also be challenging

<video controls>
  <source src="ext-figures/WaterJet.m4v" type="video/mp4">
Your browser does not support the video tag.
</video>


We can say that it looks like, but many pieces of quantitative information are difficult to extract
- how fast is it going?
- how many particles are present?
- are their sizes constant?
- are some moving faster?
- are they rearranging?


Scientific Goals
===

### Rheology

***

### Deformation


Experiments
===

The first step of any of these analyses is proper experimental design. Since there is always
- a limited field of view
- a voxel size
- a maximum rate of measurements
- a non-zero cost for each measurement

*** 

There are always trade-offs to be made between getting the best possible high-resolution nanoscale dynamics and capturing the system level behavior. 
- If we measure too fast
 - sample damage
 - miss out on long term changes
 - have noisy data
- Too slow
 - miss small, rapid changes
 - blurring and other motion artifacts
- Too high resolution
 - not enough unique structures in field of view to track
- Too low resolution
 - not sensitive to small changes

Simulation
===

In many cases, experimental data is inherited and little can be done about the design, but when there is still the opportunity, simulations provide a powerful tool for tuning and balancing a large number parameters

Simulations also provide the ability to pair post-processing to the experiments and determine the limits of tracking.


What do we start with?
===
Going back to our original cell image

1. We have been able to get rid of the noise in the image and find all the cells (lecture 2-4)
1. We have analyzed the shape of the cells using the shape tensor (lecture 5)
1. We even separated cells joined together using Watershed (lecture 6)
1. We have created even more metrics characterizing the distribution (lecture 7)

We have at least a few samples (or different regions), large number of metrics and an almost as large number of parameters to _tune_


### How do we do something meaningful with it?

Basic Simulation
===

We start with a starting image
```{r sphereimages}
# Fill Image code
# ... is for extra columns in the data set
fill.img.fn<-function(in.img,step.size=1,...) {
  xr<-range(in.img$x)
  yr<-range(in.img$y)
  ddply(expand.grid(x=seq(xr[1],xr[2],step.size),
                  y=seq(yr[1],yr[2],step.size)),
        .(x,y),
      function(c.pos) {
        ix<-c.pos$x[1]
        iy<-c.pos$y[1]
        nset<-subset(in.img,x==ix & y==iy)
        if(nrow(nset)<1) nset<-data.frame(x=ix,y=iy,val=0,...)
        nset
        })
}
make.spheres<-function(sph.list,base.gr=seq(-1,1,length.out=40)) {
  start.image<-expand.grid(x=base.gr,y=base.gr)
  start.image$val<-c(0)
  for(i in 1:nrow(sph.list)) {
    start.image$val<-with(start.image,
                          val + (
                            ((x-sph.list[i,"x"])^2+(y-sph.list[i,"y"])^2)<
                              sph.list[i,"r"]^2)
      )
  }
  start.image$phase<-with(start.image,ifelse(val>0,TRUE,FALSE))
  start.image
}
rand.list<-function(n.pts,r=0.15) data.frame(x=runif(n.pts,min=-1),y=runif(n.pts,min=-1),r=r)
grid.list<-function(n.pts,r=0.15) cbind(expand.grid(x=seq(-1,1,length.out=n.pts),y=seq(-1,1,length.out=n.pts)),r=r)
```
- A number of sphere objects with the same radius scattered evenly across the field of view
```{r, fig.height=7}
test.grid<-grid.list(5)
ggplot(subset(make.spheres(test.grid),phase),aes(x,y,fill=phase))+
  geom_raster(fill="gray50",alpha=0.75)+
  coord_equal()+
  theme_bw(20)
```

***

### Analysis

- Threshold
- Component Label
- Shape Analysis
- Distribution Analysis


Describing Motion
===
$$ \vec{v}(\vec{x})=\langle 0,0.1 \rangle $$

```{r, fig.height=7}
ggplot(subset(make.spheres(test.grid),phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  geom_segment(data=cbind(test.grid,xv=0,yv=0.1),
               aes(xend=x+xv,yend=y+yv),arrow=arrow(length = unit(0.3,"cm")))+
  coord_equal()+
  theme_bw(20)
```

***

$$ \vec{v}(\vec{x})=0.3\frac{\vec{x}}{||\vec{x}||}\times \langle 0,0,1 \rangle $$

```{r, fig.height=7}
test.grid$xyr<-with(test.grid,sqrt(x^2+y^2))
ggplot(subset(make.spheres(test.grid),phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  geom_segment(data=with(test.grid,cbind(test.grid,xv=-0.3*y/xyr,yv=0.3*x/xyr)),
               aes(xend=x+xv,yend=y+yv),arrow=arrow(length = unit(0.3,"cm")))+
  coord_equal()+
  theme_bw(20)
```


Many Frames
===

$$ \vec{v}(\vec{x})=\langle 0,0.1 \rangle $$

```{r, fig.height=9}
many.frames<-ldply(seq(0,2,length.out=9),function(in.offset) {
  cbind(
    make.spheres(data.frame(x=test.grid$x,y=test.grid$y+in.offset,r=test.grid$r)),
    offset=in.offset
    )
})

ggplot(subset(many.frames,phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  coord_equal()+
  facet_wrap(~offset)+
  labs(title="Different Frames in Linear Flow Image")+
  theme_bw(20)
```

***

$$ \vec{v}(\vec{x})=0.3\frac{\vec{x}}{||\vec{x}||}\times \langle 0,0,1 \rangle $$

```{r, fig.height=9,results='asis'}
many.frames<-cbind(make.spheres(test.grid),offset=0)
last.frame<-test.grid
for(in.offset in seq(0,2,length.out=9)) {
  last.frame$xyr<-with(last.frame,sqrt(x^2+y^2))
  last.frame$xyr[which(last.frame$xyr==0)]<-1
  last.frame<-with(last.frame,data.frame(x=x-0.05*y/xyr,y=y+0.05*x/xyr,r=r))
  
  many.frames<-rbind(many.frames,
                     cbind(make.spheres(last.frame),offset=in.offset)
                     )
}
ggplot(subset(many.frames,phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  coord_equal()+
  facet_wrap(~offset)+
  labs(title="Different Frames in Spiral Flow")+
  theme_bw(20)
```

Random Appearance / Disappearance
===

Under perfect imaging and experimental conditions objects should not appear and reappear but due to 
- noise
- limited fields of view / depth of field
- discrete segmentation approachs
- motion artifacts
 - blurred objects often have lower intensity values than still objects

It is common for objects to appear and vanish regularly in an experiment.

***

```{r, fig.height=9}
many.frames<-ldply(seq(0,1.,length.out=9),function(in.offset) {
  c.grid<-test.grid[sample(nrow(test.grid), 18), ]
  cbind(
    make.spheres(data.frame(x=c.grid$x,y=c.grid$y+in.offset,r=c.grid$r)),
    offset=in.offset
    )
})
ggplot(subset(many.frames,phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  coord_equal()+
  facet_wrap(~offset)+
  labs(title="Different Frames in Linear Flow Image")+
  theme_bw(20)
```


Jitter / Motion Noise
===
Even perfect spherical objects do not move in a straight line. The jitter can be seen as a stochastic variable with a random magnitude ($a$) and angle ($b$). This is then sampled at every point in the field

$$ \vec{v}(\vec{x})=\vec{v}_L(\vec{x})+||a||\measuredangle b $$

```{r, fig.height=7}
ggplot(subset(make.spheres(test.grid),phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  geom_segment(data=cbind(test.grid,
                          xv=0+runif(nrow(test.grid),min=-0.05,max=0.05),
                          yv=0.1+runif(nrow(test.grid),min=-0.05,max=0.05)),
               aes(xend=x+xv,yend=y+yv),arrow=arrow(length = unit(0.3,"cm")))+
  coord_equal()+
  theme_bw(20)
```

Jitter (Continued)
===

Over many frames this can change the path significantly

```{r, fig.height=9}
last.frame<-test.grid[,c("x","y","r")]
last.frame$id<-1:nrow(last.frame)
many.frames<-cbind(make.spheres(last.frame),offset=0)
many.grids<-cbind(last.frame,offset=0)

for(in.offset in cumsum(rep(0.2,8))) {
  last.frame<-with(last.frame,
                   data.frame(x=x+runif(nrow(last.frame),min=-0.1,max=0.1),
                              y=y+0.2+runif(nrow(last.frame),min=-0.1,max=0.1),
                              r=r)
                   )
  
  last.frame$id<-1:nrow(last.frame)
  
  many.frames<-rbind(many.frames,
                     cbind(make.spheres(last.frame),offset=in.offset)
                     )
  
  many.grids<-rbind(many.grids,
                    cbind(last.frame,offset=in.offset)
                    )
}
ggplot(subset(many.frames,phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  coord_equal()+
  facet_wrap(~offset)+
  labs(title="Different Frames in Linear Flow Image")+
  theme_bw(20)
```

***

The simulation can be represented in a more clear fashion by using single lines to represent each spheroid 
```{r, fig.height=9}
ggplot(many.grids,aes(x,y,))+
  geom_path(aes(color=id,group=id))+
  coord_equal()+
  labs(title="Different Paths in Linear Jittered Flow Image")+
  scale_color_gradientn(colours=rainbow(10))+
  theme_bw(20)
```

Limits of Tracking
===

We see that visually tracking samples can be difficult and there are a number of parameters which affect the ability for us to clearly see the tracking.
- flow rate
- flow type
- density
- appearance and disappearance rate 
- jitter
- particle uniqueness


***

We thus try to quantify the limits of these parameters for different tracking methods in order to design experiments better. 

### Acquisition-based Parameters

- acquisition rate $\rightarrow$ flow rate, jitter (per frame)
- resolution $\rightarrow$ density, appearance rate


### Experimental Parameters

- experimental setup (pressure, etc) $\rightarrow$ flow rate/type 
- polydispersity $\rightarrow$ particle uniqueness
- vibration/temperature $\rightarrow$ jitter
- mixture $\rightarrow$ density

Tracking: Nearest Neighbor
===
While there exist a number of different methods and complicated approaches for tracking, for experimental design it is best to start with the simplist, easiest understood method. The limits of this can be found and components added as needed until it is possible to realize the experiment
- If a dataset can only be analyzed with a multiple-hypothesis testing neural network model then it might not be so reliable

We then return to _nearest neighbor_ which means we track a point ($\vec{P}_0$) from an image ($I_0$) at $t_0$ to a point ($\vec{P}_1$) in image ($I_1$) at $t_1$ by 

$$ \vec{P}_1=\textrm{argmin}(||\vec{P}_0-\vec{y}|| \forall \vec{y}\in I_1) $$

Scoring Tracking
===

In the following examples we will use simple metrics for scoring fits where the objects are matched and the number of misses is counted.

There are a number of more sensitive scoring metrics which can be used, by finding the best submatch for a given particle since the number of matches and particles does not always correspond. See the papers at the beginning for more information


```{r track_code}
source('~/Dropbox/TIPL/src/R/trackingCode.R')
source('~/Dropbox/WorkRelated/TrackTopology/simFlow.R')
```

Basic Simulations
===

Input flow from simulation

$$ \vec{v}(\vec{x})=\langle 0,0,0.05 \rangle+||0.01||\measuredangle b $$
```{r basic_flow}
# Generate a simple synthetic dataset
in.frames<-generate.frames(base.rand=0.01,crop.size=c(-10,10),n.objects=20,n.frames=10,flow.rate=0.05,force.2d=T) 
all.tracks<-track.frames(in.frames,offset=c(0,0,0.1),run.offset=T,run.adaptive=T,maxVolPenalty=NA)


```

```{r, fig.height=7}
ggplot(do.call(rbind,in.frames),
       aes(x=POS_X,y=POS_Z,color=as.factor(REAL_LACUNA_NUMBER)))+
  geom_path(aes(linetype="Original"))+
  labs(x="X Position",y="Z Position",color="Object ID",title="Flow Simulation Results")+
  theme_bw(20)
```

***

Nearest Neighbor Tracking

```{r, fig.height=9}
ggplot(do.call(rbind,in.frames),
       aes(x=POS_X,y=POS_Z,color=as.factor(REAL_LACUNA_NUMBER)))+
  #geom_path(aes(linetype="Original"))+
  geom_path(data=subset(all.tracks,Matching=="No Offset"),aes(linetype="Tracked"),size=2,alpha=0.5)+
  facet_wrap(~Matching)+
  labs(x="X Position",y="Z Position",color="Object ID",title="Tracking Results")+
  theme_bw(20)
```

More Complicated Flows
===

Input flow from simulation

$$ \vec{v}(\vec{x})=\langle 0,0,0.01 \rangle+||0.05||\measuredangle b $$
```{r complicated}
# Generate a simple synthetic dataset
in.frames<-generate.frames(base.rand=0.1,crop.size=c(-10,10),n.objects=20,n.frames=20,flow.rate=0.01,
                           force.2d=T,rand.fun=function(x,min=0,max=1) rnorm(x,(min+max)/2,(max-min)/2)) 
all.tracks<-track.frames(in.frames,offset=c(0,0,0.1),run.offset=T,run.adaptive=T,maxVolPenalty=NA)
```

```{r, fig.height=7}
ggplot(do.call(rbind,in.frames),
       aes(x=POS_X,y=POS_Z,color=as.factor(REAL_LACUNA_NUMBER)))+
  geom_path(aes(linetype="Original"))+
  labs(x="X Position",y="Z Position",color="Object ID",title="Flow Simulation Results")+
  theme_bw(20)
```

***

Nearest Neighbor Tracking

```{r, fig.height=9}
ggplot(do.call(rbind,in.frames),
       aes(x=POS_X,y=POS_Z,color=as.factor(REAL_LACUNA_NUMBER)))+
  geom_path(aes(linetype="Original"))+
  geom_path(data=subset(all.tracks,Matching=="No Offset"),aes(linetype="Tracked"),size=2,alpha=0.5)+
  facet_wrap(~Matching)+
  labs(x="X Position",y="Z Position",color="Object ID",title="Tracking Results")+
  theme_bw(20)
```



Quantifying Tracking Rate
===
We can then quantify the success rate of each algorithm on the data set using the very simple match and mismatch metrics

```{r, fig.height=9}
c.tracks<-subset(all.tracks,Matching=="No Offset")
c.tracks<-all.tracks
c.subtrack<-subset(c.tracks,abs(D_REAL_LACUNA_NUMBER)>0)
ggplot(do.call(rbind,in.frames),
       aes(x=POS_X,y=POS_Z))+
  geom_path(aes(linetype="Original",color=as.factor(REAL_LACUNA_NUMBER)))+
  geom_path(data=c.tracks,aes(linetype="Tracked",color=as.factor(REAL_LACUNA_NUMBER)),size=2,alpha=0.5)+
  geom_point(data=c.subtrack,aes(shape="Missed"),size=5,alpha=0.5,color="red")+
  facet_wrap(~Matching)+
  labs(x="X Position",y="Z Position",color="Object ID",title="Tracking Results",fill="Misses")+
  theme_bw(20)

```

***
### Counting Misses
```{r, fig.height=4}
c.tracks<-subset(all.tracks,Matching=="No Offset")
c.tracks<-all.tracks
c.subtrack<-subset(c.tracks,abs(D_REAL_LACUNA_NUMBER)>0)
ggplot(do.call(rbind,in.frames),
       aes(x=POS_X,y=POS_Z))+
  stat_binhex(data=c.subtrack,bins=5,drop=F,alpha=1)+
  scale_fill_gradient2(low="white",high="red")+
  facet_wrap(~Matching)+
  labs(x="X Position",y="Z Position",color="Object ID",title="Tracking Results",fill="Misses")+
  theme_bw(20)

```

```{r, results='asis'}
pout<-function(x) paste(round(1000*(1-mean(x)))/10,"%",sep="")
match.table<-ddply.cutcols(all.tracks,.(cut_interval(sample,10)),function(all.sample) { 
    data.frame(
               BIJ_MATCHA=with(subset(all.sample,Matching=="No Offset"),pout(BIJ_MATCH)),
               BIJ_MATCHB=with(subset(all.sample,Matching=="Fix Offset"),pout(BIJ_MATCH)),
               BIJ_MATCHC=with(subset(all.sample,Matching=="Adaptive Offset"),pout(BIJ_MATCH))
               )
    })

names(match.table)<-c("Time","NN","ONN","ANN")
kable(match.table)
```



Jitter Sensitivity
===
```{r, fig.height=9,fig.width=14}
match.qual<-function(in.tracks) ddply(in.tracks,.(Matching),function(c.subset) {
  data.frame(Obj.Matched=sum(c.subset$D_REAL_LACUNA_NUMBER==0),
              Obj.Missed=sum(abs(c.subset$D_REAL_LACUNA_NUMBER)>0))
})
rand.fun.norm<-function(n,minv,maxv) rnorm(n,(maxv+minv)/2,(maxv-minv)/2)
n.iters<-20
z.vel<-0.1
n.frames<-2


jd.gen.fun<-function(c.jitter,c.obj.count,...) {
  test.data<-generate.frames(base.rand=c.jitter*z.vel,crop.size=c(-10,10),n.objects=c.obj.count,n.frames=n.frames,rand.fun=rand.fun.norm,...) 
  test.tracks<-track.frames(test.data,offset=c(0,0,z.vel),run.offset=T,run.adaptive=T,maxVolPenalty=NA)
  cbind(jitter=c.jitter,obj.count=c.obj.count,
        mean_obj_spacing=((1/c.obj.count)^0.33)/z.vel,
        match.qual(test.tracks))
}
jitter.vals<-rep(seq(0,2.5,length.out=15),n.iters)
jitter.full<-
jitter.summary.3d<-ddply(ldply(jitter.vals,function(c.jitter) jd.gen.fun(c.jitter,20),.parallel=T),
                      .(jitter,Matching),
                      function(c.subset) {
  data.frame(Matched=100*sum(c.subset$Obj.Matched)/(sum(c.subset$Obj.Missed)+sum(c.subset$Obj.Matched)))
})
jitter.summary.2d<-ddply(ldply(jitter.vals,function(c.jitter) jd.gen.fun(c.jitter,20,force.2d=T),.parallel=T),
                      .(jitter,Matching),
                      function(c.subset) {
  data.frame(Matched=100*sum(c.subset$Obj.Matched)/(sum(c.subset$Obj.Missed)+sum(c.subset$Obj.Matched)))
})
 ggplot(rbind(cbind(jitter.summary.3d,geom="3D"),cbind(jitter.summary.2d,geom="2D")),
        aes(x=100*jitter,y=Matched,color=geom))+
  geom_line()+geom_point()+facet_wrap(~Matching)+
  theme_bw(24)+labs(x="Position Jitter (% of Velocity)",y="% of Bubbles Matched",color="Matching Type")
```

Density and Jitter
===
```{r dj-simulation}
n.iters<-20
registerDoMC(8) # divide the jobs better

jitter.vals<-seq(0,2,length.out=8)
irseq<-function(a,b,length.out) {1/seq(1/b^(1/3),1/a^(1/3),length.out=length.out)^3} # seq for inverted numbers
obj.count<-irseq(25,2500,length.out=6)

jit.bub<-merge(obj.count,jitter.vals)
jd.vals<-mapply(list,rep(jit.bub[,1],n.iters),rep(jit.bub[,2],n.iters),SIMPLIFY=F)

jd.full<-ldply(jd.vals,.parallel=T,
               function(c.in) jd.gen.fun(c.in[[2]],c.in[[1]]))
jd.summary<-ddply(jd.full,.(jitter,mean_obj_spacing,Matching),function(c.subset) {
  data.frame(obj.count=c.subset$obj.count[1],
             Matched=100*sum(c.subset$Obj.Matched)/(sum(c.subset$Obj.Missed)+sum(c.subset$Obj.Matched)),
             obj.matched=sum(c.subset$Obj.Matched),
             obj.found=100*with(c.subset,sum(Obj.Matched)/(n.iters*(n.frames-1)*obj.count[1])))
})
```
```{r, fig.height=9,fig.width=7}
 ggplot(jd.summary,aes(x=100*jitter,y=Matched,color=as.factor(round(100*mean_obj_spacing))))+
  geom_line()+geom_point()+facet_wrap(~Matching)+
  theme_bw(24)+labs(x="Position Jitter (% of Velocity)",y="% of Obj Matched",color="Obj.Spacing\n(% of Velocity)")
```

***

```{r, fig.height=9,fig.width=7}
ggplot(jd.summary,aes(x=100*jitter,y=100*mean_obj_spacing,fill=Matched))+
  geom_tile()+facet_wrap(~Matching)+
  labs(x="Position Jitter (% of Velocity)",fill="% of Obj Matched",y="Obj.Spacing (% of Velocity)")+
  theme_bw(24)

```


Extending Nearest Neighbor
===

### Bijective Requirement
$$ \vec{P}_f=\textrm{argmin}(||\vec{P}_0-\vec{y} || \forall \vec{y}\in I_1) $$
$$ \vec{P}_i=\textrm{argmin}(||\vec{P}_f-\vec{y} || \forall \vec{y}\in I_0) $$

$$ \vec{P}_i \stackrel{?}{=} \vec{P}_0 $$

### Maximum Displacement

$$ \vec{P}_1=\begin{cases} 
||\vec{P}_0-\vec{y} ||<\textrm{MAXD}, & \textrm{argmin}(||\vec{P}_0-\vec{y} || \forall \vec{y}\in I_1) \\
\textrm{Otherwise}, & \emptyset \end{cases}$$


***

### Prior / Expected Movement

$$ \vec{P}_1=\textrm{argmin}(||\vec{P}_0+\vec{v}_{offset}-\vec{y} || \forall \vec{y}\in I_1) $$

### Adaptive Movement
Can then be calculated in an iterative fashion where the offset is the average from all of the $\vec{P}_1-\vec{P}_0$ vectors. It can also be performed 
$$ \vec{P}_1=\textrm{argmin}(||\vec{P}_0+\vec{v}_{offset}-\vec{y} || \forall \vec{y}\in I_1) $$


Beyond Nearest Neighbor
===

While nearest neighbor provides a useful starting tool it is not sufficient for truly complicated flows and datasets.
### Better Approaches

1. Multiple Hypothesis Testing
Nearest neighbor just compares the points between two frames and there is much more information available in most time-resolved datasets. This approach allows for multiple possible paths to be explored at the same time and the best chosen only after all frames have been examined

***

### Shortcomings
1. Merging and Splitting Particles
The simplicity of the nearest neighbor model does really allow for particles to merge and split (relaxing the bijective requirement allows such behavior, but the method is still not suited for such tracking). For such systems a more specific, physically-based is required to encapsulate this behavior.


Voxel-based Approaches
===
For voxel-based approachs the most common analyses are digital image correlation (or for 3D images digital volume correlation), where the correlation is calculated between two images or volumes.


Standard Image Correlation
===
Given images $I_0(\vec{x})$ and $I_1(\vec{x})$ at time $t_0$ and $t_1$ respectively. The correlation between these two images can be calculated

$$ C_{I_0,I_1}(\vec{r})=\langle I_0(\vec{x}) I_1(\vec{x}+\vec{r}) \rangle $$

```{r, fig.height=5}
start.grid<-grid.list(5)
start.img<-make.spheres(start.grid,base.gr=seq(-1,1,length.out=40))

final.grid<-data.frame(x=with(start.grid,x),y=with(start.grid,y+0.2),r=start.grid$r)
final.img<-make.spheres(final.grid,base.gr=seq(-1,1,length.out=40))

ggplot(subset(start.img,phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  coord_equal()+
  theme_bw(20)
```

***

```{r, fig.height=5}
ggplot(subset(final.img,phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  coord_equal()+
  theme_bw(20)
```

```{r cross_corrfun}
#' Calculate the cross correlation 
#' @author Kevin Mader (kevin.mader@gmail.com)
#' Generates flow with given object count, frame count and randomness
#' the box and crop are introduced to allow for objects entering and 
#' leaving the field of view
#'
#' @param img.a is the starting or I_0 image
#' @param img.b is the destination or I_1 image
#' @param tr.x is the function transforming the x coordinate
#' 
cc.imfun<-function(img.a,img.b,tr.x=function(x,y) x,tr.y=function(x,y) y) {
  tr.img.b<-img.b
  tr.img.b$x<-with(img.b,tr.x(x,y))
  tr.img.b$y<-with(img.b,tr.y(x,y))
  matches<-ddply(rbind(cbind(img.a,label="A"),cbind(tr.img.b,label="B")),.(x,y), function(c.pos) {
    if(nrow(c.pos)>1) data.frame(e.val=prod(c.pos$phase))
    else data.frame(e.val=c())
  })
  data.frame(e.val=mean(matches$e.val),count=nrow(matches))
}
```

```{r }
gr.spacing<-diff(unique(start.img$x))[1]
cc.points<-expand.grid(vx=seq(-5,5,1)*gr.spacing,vy=seq(-5,5,1)*gr.spacing)
cc.img<-ddply(cc.points,.(vx,vy),function(c.pt) {
  tr.x<-function(x,y) (x+c.pt[1,"vx"])
  tr.y<-function(x,y) (y+c.pt[1,"vy"])
  cc.imfun(start.img,final.img,tr.x=tr.x,tr.y=tr.y)
},.parallel=T)
```

```{r, fig.height=5}
ggplot(cc.img,aes(vx,vy,fill=e.val))+
  geom_raster()+geom_density2d()+
  labs(x="u",y="v",fill="Correlation",title="Correlation vs R")+
  scale_fill_gradient2(high="red")+
  theme_bw(25)
```


Random Image Positions
===
With highly structured / periodic samples identfying a best correlation is difficult since there are multiple maxima. 

```{r, fig.height=5}
start.grid<-rand.list(5)
start.img<-make.spheres(start.grid,base.gr=seq(-1,1,length.out=40))

final.grid<-data.frame(x=with(start.grid,x),y=with(start.grid,y+0.2),r=start.grid$r)
final.img<-make.spheres(final.grid,base.gr=seq(-1,1,length.out=40))

ggplot(subset(start.img,phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  coord_equal()+
  theme_bw(20)
```

***

```{r, fig.height=5}
ggplot(subset(final.img,phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  coord_equal()+
  theme_bw(20)
```

```{r, fig.height=5}
gr.spacing<-diff(unique(start.img$x))[1]
cc.points<-expand.grid(vx=seq(-5,5,1)*gr.spacing,vy=seq(-5,5,1)*gr.spacing)
cc.img<-ddply(cc.points,.(vx,vy),function(c.pt) {
  tr.x<-function(x,y) (x+c.pt[1,"vx"])
  tr.y<-function(x,y) (y+c.pt[1,"vy"])
  cc.imfun(start.img,final.img,tr.x=tr.x,tr.y=tr.y)
},.parallel=T)
```

```{r, fig.height=5}
ggplot(cc.img,aes(vx,vy,fill=e.val))+
  geom_raster()+geom_density2d()+
  labs(x="u",y="v",fill="Correlation",title="Correlation vs R")+
  scale_fill_gradient2(high="red")+
  theme_bw(25)
```

Extending Correlation
===

### Scaling and Rotation Generalization
$$ C_{I_0,I_1}(\vec{r},s,\theta)= $$
$$ \langle I_0(\vec{x}) I_1(
\begin{bmatrix}
s\cos\theta & -s\sin\theta\\
s\sin\theta & s\cos\theta
\end{bmatrix}
\vec{x}+\vec{r}) \rangle  $$

More Complicated Changes
===

```{r, fig.height=7}
start.grid<-grid.list(5)
start.img<-make.spheres(start.grid,base.gr=seq(-1,1,length.out=100))

final.grid<-data.frame(x=with(start.grid,sign(x)*abs(x)^1.5),y=with(start.grid,sign(y)*abs(y)^1.5),r=start.grid$r)
final.img<-make.spheres(final.grid,base.gr=seq(-1,1,length.out=100))

ggplot(subset(start.img,phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  coord_equal()+
  theme_bw(20)
```

***

```{r, fig.height=7}
ggplot(subset(final.img,phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  coord_equal()+
  theme_bw(20)
```


Choosing a window size
===
```{r, fig.height=7}
blockify.img<-function(in.img,nx=5,ny=5) {
  out.img<-ddply(in.img,.(cut_interval(x,nx),cut_interval(y,ny)),function(c.block) c.block)
  names(out.img)[c(1:2)]<-c("x.block","y.block")
  out.img$label.block<-with(out.img,paste(x.block,y.block,sep=","))
  out.img
}
start.img.blocks<-blockify.img(start.img,4,4)
ggplot(subset(start.img.blocks,phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  coord_equal()+facet_wrap(~label.block,scale="free")+
  theme_bw(10)
```

***
```{r, fig.height=7}
final.img.blocks<-blockify.img(final.img,4,4)
ggplot()+
  geom_raster(data=subset(start.img.blocks,phase),aes(x,y,fill="Start"),alpha=0.75)+
  geom_raster(data=subset(final.img.blocks,phase),aes(x,y,fill="Final"),alpha=0.75)+
  coord_equal()+facet_wrap(~label.block,scale="free")+
  labs(fill="Time")+
  theme_bw(10)
```

Too large of a window
===
```{r, fig.height=7}
start.img.blocks<-blockify.img(start.img,2,2)
ggplot(subset(start.img.blocks,phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  coord_equal()+facet_wrap(~label.block,scale="free")+
  theme_bw(10)
```

***
```{r, fig.height=7}
final.img.blocks<-blockify.img(final.img,2,2)
ggplot()+
  geom_raster(data=subset(start.img.blocks,phase),aes(x,y,fill="Start"),alpha=0.75)+
  geom_raster(data=subset(final.img.blocks,phase),aes(x,y,fill="Final"),alpha=0.75)+
  coord_equal()+facet_wrap(~label.block,scale="free")+
  labs(fill="Time")+
  theme_bw(10)
```

Too small of a window
===
```{r, fig.height=7}
start.img.blocks<-blockify.img(start.img,6,6)
ggplot(subset(start.img.blocks,phase),aes(x,y))+
  geom_raster(fill="gray50",alpha=0.75)+
  coord_equal()+facet_wrap(~label.block,scale="free")+
  theme_bw(10)
```

***
```{r, fig.height=7}
final.img.blocks<-blockify.img(final.img,6,6)
ggplot()+
  geom_raster(data=subset(start.img.blocks,phase),aes(x,y,fill="Start"),alpha=0.75)+
  geom_raster(data=subset(final.img.blocks,phase),aes(x,y,fill="Final"),alpha=0.75)+
  coord_equal()+facet_wrap(~label.block,scale="free")+
  labs(fill="Time")+
  theme_bw(10)
```

