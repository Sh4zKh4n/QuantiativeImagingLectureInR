```{r global_setup,  warning=FALSE, cache=FALSE,echo=FALSE,error=FALSE,results='hide', message=F}
source("../common/slide_setup.R")
```

```{r script_setup, cache=F}
## The basic files and libraries needed for most presentations
# creates the libraries and common-functions sections
read_chunk("../common/utility_functions.R")
```
```{r libraries}
```
```{r common-functions}
```

Quantitative Big Imaging 
========================================================
author: Kevin Mader, Christian Dietz
date: 19 February 2015
width: 1440
height: 900
css: ../common/template.css
transition: rotate



ETHZ: 227-0966-00L
# Introductions and Workflows

Course Outline
========================================================

```{r, results='asis'}
source('../common/schedule.R')
```


Who are we?
===
- Kevin Mader (mader@biomed.ee.ethz.ch)
 - __Lecturer__ at ETH Zurich 
 - __Postdoc__ in the X-Ray Microscopy Group at ETH Zurich and Swiss Light Source at Paul Scherrer Institute
 - Spin-off __4Quant__ for Big Data with Images
 
![Kevin Mader](../common/figures/kevin-mader.jpg)
 
***

- Marco Stampanoni (marco.stampanoni@psi.ch)
 - __Professor__ at ETH Zurich
 - __Group Leader__ for the X-Ray Microscopy Group at ETH Zurich and Swiss Light Source at Paul Scherrer Institute
 
![Marco Stampanoni](../common/figures/marco.png)

Who are we (continued)?
===

- Anders Kaestner (anders.kaestner@psi.ch) 
 - __Group Leader__ at the ICON Beamline at the SINQ (Neutron Source) at Paul Scherrer Institute

![Anders Kaestner](../common/figures/anders.png)

Who are we (continued)?
===
- Filippo Arcadu (filippo.arcadu@psi.ch)
 - Exercise assistance
 - __PhD Student__ in the X-Ray Microscopy Group at ETH Zurich and Swiss Light Source at Paul Scherrer Institute
 
![Filippo Arcadu](../common/figures/filippo.png)
 
***

- Christian Dietz (christian.dietz@uni-konstanz.de)
 - KNIME Expert
 - __PhD Student__ Fachbereich Informatik und Informationswissenschaft
 - __KNIME Developer__ KNIME Image Processing, ImageJ2, imglib (https://github.com/dietzc)

![Christian Dietz](../common/figures/christian-dietz.png)

Who are you?
===
### A wide spectrum of backgrounds
- Biomedical Engineers, Physicists, Chemists, Art History Researchers, Mechanical Engineers, and Computer Scientists

### A wide range of skills
- I think I've heard of Matlab before $\rightarrow$ I write template C++ code and hand optimize it afterwards

***

### So how will this ever work?
### Adaptive assignments

1. Conceptual, graphical assignments with practical examples
  - Emphasis on chosing correct steps and understanding workflow
2. Opportunities to create custom implementations, plugins, and perform more complicated analysis on larger datasets if interested
  - Emphasis on performance, customizing analysis, and scalability


Literature / Useful References
========================================================
### Books
- Jean Claude, Morphometry with R
 - [Online](http://link.springer.com/book/10.1007%2F978-0-387-77789-4) through ETHZ
 - [Buy it](http://www.amazon.com/Morphometrics-R-Use-Julien-Claude/dp/038777789X)
- John C. Russ, “The Image Processing Handbook”,(Boca Raton, CRC Press)
 - Available [online](http://dx.doi.org/10.1201/9780203881095) within domain ethz.ch (or proxy.ethz.ch / public VPN) 
- J. Weickert, Visualization and Processing of Tensor Fields
 - [Online](http://books.google.ch/books?id=ScLxPORMob4C&lpg=PA220&ots=mYIeQbaVXP&dq=&pg=PA220#v=onepage&q&f=false)

Motivation
===

![Crazy Workflow](../common/figures/crazyworkflow.png)
- To understand what, why and how from the moment an image is produced until it is finished (published, used in a report, …)
- To learn how to go from one analysis on one image to 10, 100, or 1000 images (without working 10, 100, or 1000X harder)

Motivation (Why does this class exist?)
===
- Detectors are getting bigger and faster constantly
- Todays detectors are really fast
 - 2560 x 2160 images @ 1500+ times a second = 8GB/s
- Matlab / Avizo / Python / … are saturated after 60 seconds
- A single camera
 - More information per day than Facebook*
 - Three times as many images per second as  Instagram**
 
<small>
- *http://news.cnet.com/8301-1023_3-57498531-93/facebook-processes-more-than-500-tb-of-data-daily/
- **http://techcrunch.com/2013/01/17/instagram-reports-90m-monthly-active-users-40m-photos-per-day-and-8500-likes-per-second/
</small>


Motivation (Is it getting better?)
===

1. __Experimental Design__ finding the right technique, picking the right dyes and samples has stayed relatively consistent, better techniques lead to more demanding scientits.

2. __Management__ storing, backing up, setting up databases, these processes have become easier and more automated as data magnitudes have increased

3. __Measurements__ the actual acquisition speed of the data has increased wildly due to better detectors, parallel measurement, and new higher intensity sources

4. __Post Processing__ this portion has is the most time-consuming and difficult and has seen minimal improvements over the last years 

***

```{r time-figure, fig.width=12, fig.height=8}
library("ggthemes")
# guesstimates
time.data<-data.frame(
  year=c(2000,2008,2014,2020),
  "Experiment Design"=c(10,10,8,8),
  "Measurements"=c(50,5,0.5,0.1),
  "Management"=c(20,15,10,8),
  "Post Processing"=c(50,50,50,50)
  )
mtd<-ddply(melt(time.data,id.vars="year"),.(year),
           function(x) cbind(x,sum.val=sum(x$value),norm.val=100*x$value/sum(x$value))
           )
mtd$variable<-gsub("[.]"," ",mtd$variable)
ggplot(mtd,aes(x=as.factor(year),y=norm.val,fill=variable))+
  geom_bar(stat="identity")+
  labs(x="Year",y="Relative Time",fill="",title="Experimental Time Breakdown")+
  theme_economist(20)
```

Saturating Output
===

```{r scaling, fig.width=12}
output.df<-data.frame(Year=time.data$year,
                      Measurements=
                        365*24/(time.data$Experiment.Design+time.data$Measurements),
                           Publications=365*24/rowSums(time.data[,c(2:5)])
                           )
mtd2<-melt(output.df,id.vars="Year")
ggplot(mtd2,aes(x=as.factor(Year),y=value,fill=variable))+
  geom_bar(stat="identity",position="dodge")+
  scale_y_sqrt()+
  labs(x="Year",y="Output",fill="")+
  theme_wsj(25)
```



```{r, results='asis'}
kable(output.df,digits=0)
```

***
To put more real numbers on these scales rather than 'pseudo-publications', the time to measure a terabyte of data is shown in minutes.

```{r, results='asis'}
mmtb<-data.frame(
  Year=c(2000,2008,2014,2016),
  y=1024/c(1/4,60/64,32,8*60)
  )
names(mmtb)[2]<-"Time to 1 TB in Minutes"
kable(mmtb,digits=0)
```

How much is a TB, really?
===

If __you__ looked at one 1000 x 1000 sized image
```{r}
image(255*matrix(runif(1000*1000),nrow=1000))
```
every second, it would take you  
```{r, results='asis'} 
# assuming 16 bit images and a 'metric' terabyte
time.per.tb<-1e12/(1000*1000*16/8) / (60*60)
cat("__",
  round(time.per.tb),
  "__",sep="")
```
hours to browse through a terabyte of data.


***

```{r, results='asis'}
names(mmtb)[2]<-"Time to 1 TB"
mmtb$"Man power to keep up"<-time.per.tb*60/mmtb$"Time to 1 TB"
mmtb[,2]<-paste(round(mmtb[,2]),"min") # add minutes suffix
mmtb$"Salary Costs / Month"<-mmtb$"Man power to keep up"*12500
mmtb$"Man power to keep up"<-paste(round(mmtb$"Man power to keep up")," people")
mmtb$"Salary Costs / Month"<-paste(round(mmtb$"Salary Costs / Month"/1000)," kCHF")
kable(mmtb,digits=0)
```

Overwhelmed
===
- Count how many cells are in the bone slice
- Ignore the ones that are ‘too big’ or shaped ‘strangely’
- Are there more on the right side or left side?
- Are the ones on the right or left bigger, top or bottom?

***

![cells in bone tissue](../common/figures/bone-cells.png)

More overwhelmed
===

- Do it all over again for 96 more samples, this time with 2000 slices instead of just one!

***

![more samples](../common/figures/96-samples.png)

Kill me now
===

- Now again with 1090 samples!

***

![even more samples](../common/figures/1090-samples.png)


It gets better
===

- Those metrics were quantitative and could be easily visually extracted from the images
- What happens if you have _softer_ metrics

***

![alignment](../common/figures/alignment-figure.png)


- How aligned are these cells?
- Is the group on the left more or less aligned than the right?
- errr?

Dynamic Information
===

<video controls>
  <source src="../common/movies/dk31-plat.avi" type="video/avi">
Your browser does not support the video tag.
</video>

***

- How many bubbles are here?
- How fast are they moving?
- Do they all move the same speed?
- Do bigger bubbles move faster?
- Do bubbles near the edge move slower?
- Are they rearranging?


Computing has changed: Parallel
===

### Moores Law
$$ \textrm{Transistors} \propto 2^{T/(\textrm{18 months})} $$

```{r invented, fig.cap="Based on trends from Wikipedia and Intel"}
# stolen from https://gist.github.com/humberto-ortiz/de4b3a621602b78bf90d
moores.txt<-c("Id Name  Year  Count(1000s)  Clock(MHz)\n",
        "0            MOS65XX  1975           3.51           14\n",
        "1          Intel8086  1978          29.00           10\n",
        "2          MIPSR3000  1988         120.00           33\n",
        "3           AMDAm486  1993        1200.00           40\n",
        "4        NexGenNx586  1994        3500.00          111\n",
        "5          AMDAthlon  1999       37000.00         1400\n",
        "6   IntelPentiumIII  1999       44000.00         1400\n",
        "7         PowerPC970  2002       58000.00         2500\n",
        "8       AMDAthlon64  2003      243000.00         2800\n",
        "9    IntelCore2Duo  2006      410000.00         3330\n",
        "10         AMDPhenom  2007      450000.00         2600\n",
        "11      IntelCorei7  2008     1170000.00         3460\n",
        "12      IntelCorei5  2009      995000.00         3600")
moores.pt.text<-gsub("zZ","\n",gsub("\\s+",",",paste(moores.txt,collapse="zZ")))
moores.df<-read.csv(text=moores.pt.text)
names(moores.df)[c(4,5)]<-c("kTransistors","Speed")
moores.df<-moores.df[,c(2:5)]

moores.table<-melt(moores.df,
  id.vars=c("Year","Name"))
moores.law<-function(year) moores.df[1,"kTransistors"]*2^((year-moores.df[1,"Year"])/1.5)
moores.table$variable<-gsub("Speed","Speed (MHz)",moores.table$variable)
ggplot(moores.table,aes(Year,value,color=variable))+
  geom_line(aes(linetype="Moore's Law",y=moores.law(moores.table$Year)),color="black",
            size=2,alpha=0.8)+
  geom_jitter()+  
  geom_smooth()+

  scale_y_log10()+
  labs(color="",y="",linetype="")+
  theme_economist(20)
```
<small>_Based on data from https://gist.github.com/humberto-ortiz/de4b3a621602b78bf90d_</small>

***

There are now many more transistors inside a single computer but the processing speed hasn't increased. How can this be?

- Multiple Core
 - Many machines have multiple cores for each processor which can perform tasks independently
- Multiple CPUs
 - More than one chip is commonly present
- New modalities
  - GPUs provide many cores which operate at slow speed

### Parallel Code is important

Computing has changed: Cloud
===
```{r cloud-code}
cloudCostGen<-function(cloudPeakCost,cloudSpotCost) function(hours,peakRatio=1) 
  hours*peakRatio*cloudPeakCost+hours*(1-peakRatio)*cloudSpotCost
localCostGen<-function(compCost,energyPriceStd,compPowerRaw) {
  energyPrice<-energyPriceStd/1e6 # $ / Whr
  function(hours)
    compCost+energyPrice*compPowerRaw*hours
}

make.res.table<-function(cloudCost,localCost,  years) {

  ldply(years,function(life) {
    timef<-seq(0,life*24*365,length.out=50)
    data.frame(life=life*12,hours=timef,
               hoursPerWeek=timef/(life*52),
               WorstCaseCloud=cloudCost(timef,1),
               BestCaseCloud=cloudCost(timef,0),
               MiddleCaseCloud=cloudCost(timef,0.5),
               LocalWorkstation=localCost(timef))
  })
}

random.sim<-function(cloudCost,localCost,n.guess) {
  utility<-runif(n.guess,min=0,max=1)
  serviceYears<-runif(n.guess,min=1.5,max=6.5)
  peak<-runif(n.guess,min=0,max=1)
  hours<-serviceYears*utility*24*365
  ot<-data.frame(hours=hours,
             years=serviceYears,
             months=serviceYears*12,
             ryears=round(serviceYears),
             utility=utility,
             peak=peak,
             clcost = cloudCost(hours,peak),
             wscost = localCost(hours)
  )
  ot$cloudPremium<-with(ot,clcost/wscost*100)
  ot
}

scen.fcn<-function(c.pts) {
  data.frame(
    clcost=mean(c.pts$clcost),
    wscost=mean(c.pts$wscost)
  )
}

scen.grid<-function(scen.table.raw) {
  ot<-ddply.cutcols(scen.table.raw,.(cut_interval(utility,7),
                                     cut_interval(peak,7),
                                     cut_interval(months,4)),cols=3,
                    scen.fcn
  )
  ot$cloudPremium<-with(ot,clcost/wscost*100)
  ot
}

scen.table<-function(scen.table.raw) {
  ot<-ddply.cutcols(scen.table.raw,.(cut_interval(utility,25),
                                     cut_interval(peak,5),
                                     cut_interval(months,5)),cols=3,
                    scen.fcn
  )
  ot$cloudPremium<-with(ot,clcost/wscost*100)
  
  ddply(ot,.(peak,months),function(c.pts) {
    n.pts<-subset(c.pts,clcost<=wscost)
    subset(n.pts,utility==max(n.pts$utility))
  })
}
```
```{r cloud-costs}
compCost<-3999.99 # http://www.bestbuy.com/site/cybertronpc-desktop-intel-core-i7-64gb-memory-2tb-hdd-120gb-solid-state-drive-120gb-solid-state-drive-black-blue/6357048.p?id=1219209052767&skuId=6357048
compPowerRaw<-190  #Watts http://www.eu-energystar.org/en/en_008.shtml#use
energyPriceStd<-0.143 # Eu/KWHr http://www.eu-energystar.org/en/en_008.shtml#use
energyPrice<-energyPriceStd/1000*1.2 # $ / Whr
cloudPeakCost<-0.780
cloudSpotCost<-0.097

cloudCost<-function(hours,peakRatio=1) 
  hours*peakRatio*cloudPeakCost+hours*(1-peakRatio)*cloudSpotCost
localCost<-function(hours)
  compCost+energyPrice*compPowerRaw*hours
```

- Computer, servers, workstations are wildly underused (majority are <50%)
- Buying a big computer that sits idle most of the time is a waste of money

<small>http://www-inst.eecs.berkeley.edu/~cs61c/sp14/
“The Case for Energy-Proportional Computing,” Luiz André Barroso, Urs Hölzle, IEEE Computer, December 2007</small>

![cloud services](../common/figures/cloud-services.png)

***

- Traditionally the most important performance criteria was time, how fast can it be done
- With Platform as a service servers can be rented instead of bought
- Speed is still important but using cloud computing $ / Sample is the real metric
- In Switzerland a PhD student if 400x as expensive per hour as an Amazon EC2 Machine
- Many competitors keep prices low and offer flexibility




Cloud Computing Costs
===

The figure shows the range of cloud costs (determined by peak usage) compared to a local workstation with utilization shown as the average number of hours the computer is used each week.

```{r}
res.table<-
  ldply(c(3),function(life) {
    timef<-seq(0,life*24*365,length.out=50)
    data.frame(life=life*12,hours=timef,
                      hoursPerWeek=timef/(life*52),
                      WorstCaseCloud=cloudCost(timef,1),
                      BestCaseCloud=cloudCost(timef,0),
                      MiddleCaseCloud=cloudCost(timef,0.5),
                      LocalWorkstation=localCost(timef))
  })
ggplot(res.table,
       aes(x=hoursPerWeek))+
  geom_line(aes(y=LocalWorkstation,color="Local Workstation"),size=2)+
  geom_errorbar(aes(ymin=WorstCaseCloud,ymax=BestCaseCloud,color="Cloud"))+
  labs(x="Average Computer Usage (hr/wk)",y="Cost ($)",color="Solution")+
  facet_wrap(~life)+
  theme_bw(25)
```

***

The figure shows the cost of a cloud based solution as a percentage of the cost of buying a single machine. The values below 1 show the percentage as a number. The panels distinguish the average time to replacement for the machines in months

```{r}
n.guess<-1e6
utility<-runif(n.guess,min=0,max=1)
serviceYears<-runif(n.guess,min=1.5,max=6.5)
peak<-runif(n.guess,min=0,max=1)
hours<-serviceYears*utility*24*365
scen.table.raw<-data.frame(hours=hours,
                       years=serviceYears,
                       months=serviceYears*12,
                       ryears=round(serviceYears),
                       utility=utility,
                       peak=peak,
                       clcost = cloudCost(hours,peak),
                       wscost = localCost(hours)
                                  )
scen.fcn<-function(c.pts) {
                                             data.frame(
                                               clcost=mean(c.pts$clcost),
                                               wscost=mean(c.pts$wscost)
                                               )
                                             }
scen.table<-ddply.cutcols(scen.table.raw,.(cut_interval(utility,7),
                                           cut_interval(peak,7),
                                           cut_interval(months,4)),cols=3,
                          scen.fcn
                                           )
scen.table$cloudPremium<-with(scen.table,clcost/wscost*100)
ggplot(scen.table,aes(y=utility*100,x=peak*100,fill=cloudPremium))+
  geom_raster()+
  geom_text(aes(label=ifelse(cloudPremium<100,round(cloudPremium),"")))+
  labs(x="Peak Usage (%)",y="Utilization (%)",fill="Cloud Costs\n(% of Workstation)")+
  scale_fill_gradientn(colours=rainbow(4))+
  facet_wrap(~months)+
  theme_bw(10)
```

Cloud: Equal Cost Point
===

Here the equal cost point is shown where the cloud and local workstations have the same cost. The x-axis is the percentage of resources used at peak-time and the y shows the expected usable lifetime of the computer. The color indicates the utilization percentage and the text on the squares shows this as the numbers of hours used in a week.

***
```{r}
scen.table<-ddply.cutcols(scen.table.raw,.(cut_interval(utility,25),
                                           cut_interval(peak,5),
                                           cut_interval(months,5)),cols=3,
                          scen.fcn
                                           )
scen.table.sub<-ddply(scen.table,.(peak,months),function(c.pts) {
  n.pts<-subset(c.pts,clcost<=wscost)
  subset(n.pts,utility==max(n.pts$utility))
})

ggplot(scen.table.sub,aes(100*peak,months,fill=100*utility))+
  geom_raster(alpha=0.65)+
  geom_text(aes(label=round(utility*168)))+
  labs(x="% Peak Time",y="Computer Lifetime\n(Months)",fill="Utilization (%)")+
  scale_fill_gradientn(colours=rainbow(2))+
  theme_bw(20)
```

Course Overview
===
```{r, results='asis'}
source('../common/schedule.R')
```

***

### More Detail

On Imaging
===
What is an image?

Image Formation
===
![Traditional Imaging](../common/figures/traditional-imaging.png)

- __Impulses__ Light, X-Rays, Electrons, A sharp point, Magnetic field, Sound wave
- __Characteristics__ Electron Shell Levels, Electron Density, Phonons energy levels, Electronic, Spins, Molecular mobility
- __Response__ Absorption, Reflection, Phase Shift, Scattering, Emission
- __Detection__ Your eye, Light sensitive film, CCD / CMOS, Scintillator, Transducer

Where do images come from?
===

```{r, results='asis'}
in.table<-read.delim(text="Modality\tImpulse	Characteristic	Response	Detection
Light Microscopy	White Light	Electronic interactions	Absorption	Film, Camera
Phase Contrast	Coherent light	Electron Density (Index of Refraction)	Phase Shift	Phase stepping, holography, Zernike
Confocal Microscopy	Laser Light	Electronic Transition in Fluorescence Molecule	Absorption and reemission	Pinhole in focal plane, scanning detection
X-Ray Radiography	X-Ray light	Photo effect and Compton scattering	Absorption and scattering	Scintillator, microscope, camera
Ultrasound	High frequency sound waves	Molecular mobility	Reflection and Scattering	Transducer
MRI	Radio-frequency EM	Unmatched Hydrogen spins	Absorption and reemission	RF coils to detect
Atomic Force Microscopy	Sharp Point	Surface Contact	Contact, Repulsion	Deflection of a tiny mirror",header=T)
kable(in.table,caption="Various modalities and their ways of being recorder")
```

Acquiring Images
===
### Traditional / Direct imaging
- Visible images produced or can be easily made visible
- Optical imaging, microscopy

```{r, fig.cap=" here the measurement is supposed to be from a typical microscope which blurs, flips and otherwise distorts the image but the original representation is still visible"}

bone.img<-t(readPNG(qbi.file("tiny-bone.png")))
attr(bone.img,"type")<-"grey"
meas.img<-im.to.df(flip(gblur(bone.img,3)))
both.img<-rbind(
  cbind(meas.img,src="Measured"),
  cbind(im.to.df(bone.img),src="Reconstructed")
  )
ggplot(both.img,aes(x,y,fill=val))+
  geom_tile()+
  coord_equal()+
  facet_wrap(~src)+
  th_fillmap.fn(2)+
  guides(fill=F)+
  labs(x="",y="")+
  theme_bw(20)+
  theme(axis.ticks = element_blank(), 
        axis.text.x = element_blank(),
        axis.text.y = element_blank())
```

***

### Indirect / Computational imaging
- Recorded information does not resemble object
- Response must be transformed (usually computationally) to produce an image

```{r, fig.cap="here the measurement is supposed to be from a diffraction style experiment where the data is measured in reciprocal space (fourier) and can be reconstructed to the original shape"}
##
fftshift2<-function (y) 
{
    nx = nrow(y)
    nx2 = floor(nx/2)
    ny = ncol(y)
    ny2 = floor(ny/2)
    y[c((nx2+1):nx,1:nx2),c((ny2+1):ny,1:ny2)]
}
scat.img<-im.to.df(abs(fftshift2(fft(bone.img))))
scat.img<-mutate(scat.img,val=log(val)/3.5)
both.img<-rbind(
  cbind(scat.img,src="Measured"),
  cbind(im.to.df(bone.img),src="Reconstructed")
  )
ggplot(both.img,aes(x,y,fill=val))+
  geom_tile()+
  coord_equal()+
  facet_wrap(~src)+
  th_fillmap.fn(2)+
  guides(fill=F)+
  labs(x="",y="")+
  theme_bw(20)+
  theme(axis.ticks = element_blank(), 
        axis.text.x = element_blank(),
        axis.text.y = element_blank())
```

Traditional Imaging
===

![Traditional Imaging](../common/figures/traditional-imaging.png)


<small>
Copyright 2003-2013 J. Konrad in EC520 lecture, reused with permission
</small>

Traditional Imaging: Model
===

![Traditional Imaging Model](../common/figures/traditional-image-flow.png)

$$
\left[\left([b(x,y)*s_{ab}(x,y)]\otimes h_{fs}(x,y)\right)*h_{op}(x,y)\right]*h_{det}(x,y)+d_{dark}(x,y)
$$

$s_{ab}$ is the only information you are really interested in, so it is important to remove or correct for the other components

For color (non-monochromatic) images the problem becomes even more complicated
$$
\int_{0}^{\infty} {\left[\left([b(x,y,\lambda)*s_{ab}(x,y,\lambda)]\otimes h_{fs}(x,y,\lambda)\right)*h_{op}(x,y,\lambda)\right]*h_{det}(x,y,\lambda)}\mathrm{d}\lambda+d_{dark}(x,y)
$$

Common Types
===
Traditional / Direct imaging
Visible images produced or can be easily made visible
Indirect / Computational imaging
Tomography through projections
Microlenses
Diffraction patterns
Surface Topography with cantilevers (AFM)
Hyperspectral imaging with Raman, IR, CARS
…..

On Science
===
Image processing is a complicated field
- Thousands of algorithms available
- Thousands of tools
- Many images require multi-step processing
- Experimenting is time-consuming

***

Science demands __repeatability__! and really wants __reproducability__
- Experimental conditions can change rapidly and are difficult to make consistent
- Animal and human studies are prohibitively time consuming and expensive to reproduce
- Terabyte datasets cannot be easily passed around many different groups
- Privacy concerns can also limit sharing and access to data




Soup Example
===

Easy to follow the list, anyone with the right steps can execute and repeat (if not reproduce) the soup

### Simple Soup

1. Buy {carrots, peas, tomatoes} at market
1. _then_ Buy meat at butcher
1. _then_ Chop carrots into pieces
1. _then_ Chop potatos into pieces
1. _then_ Heat water
1. _then_ Wait until boiling then add chopped vegetables
1. _then_ Wait 5 minutes and add meat

***

### More complicated soup
Here it is harder to follow and you need to carefully keep track of what is being performed

### Steps 1-4
4. _then_ Mix carrots with potatos $\rightarrow  mix_1$
4. _then_ add egg to $mix_1$ and fry for 20 minutes
4. _then_ Tenderize meat for 20 minutes
4. _then_ add tomatoes to meat and cook for 10 minutes $\rightarrow mix_2$
5. _then_ Wait until boiling then add $mix_1$
6. _then_ Wait 5 minutes and add $mix_2$

Using flow charts / workflows
===

### Simple Soup
```{r, fig.height=9}
library(igraph)
node.names<-c("Buy\nvegetables","Buy meat","Chop\nvegetables","Heat water","Wait for\nboiling","Wait 5\nadd meat")
c.mat<-matrix(0,length(node.names),length(node.names))
for(i in c(1:(length(node.names)-1))) c.mat[i,i+1]<-1
colnames(c.mat)<-node.names
rownames(c.mat)<-node.names
g<-graph.adjacency(c.mat,mode="directed")
V(g)$degree <- degree(g)
V(g)$label <- V(g)$name
V(g)$color <- "lightblue"
V(g)$size<-80
E(g)$width<-2

E(g)$color<-"black"

plot(g,layout=layout.circle)#,  layout= layout.kamada.kawai) ##
```

***

### Complicated Soup

```{r, fig.height=9}
new.nodes<-c(node.names,"Mix carrots\npotatoes","add egg\nand fry","tenderize\nmeat",
             "add tomatoes\ncook 10min","wait 5 minutes")
c.mat<-matrix(0,length(new.nodes),length(new.nodes))
# old connections
for(i in c(1:(length(new.nodes)-1))) c.mat[i,i+1]<-1
colnames(c.mat)<-new.nodes
rownames(c.mat)<-new.nodes
c.mat[1,2]<-0
c.mat[1,3]<-1
c.mat[2,3]<-0
c.mat[2,9]<-1
c.mat[6,7]<-0
c.mat[6,11]<-1
# chop vegies
c.mat[3,]<-0
c.mat[3,7]<-1

c.mat[8,]<-0
c.mat[8,5]<-1

g<-graph.adjacency(c.mat,mode="directed")
V(g)$degree <- degree(g)
V(g)$label <- V(g)$name
V(g)$color <- "lightblue"
V(g)$size<-40
E(g)$width<-2

E(g)$color<-"black"

plot(g,layout=layout.circle)#,  layout= layout.kamada.kawai) ##
```

Workflows
===
Clearly a linear set of instructions is ill-suited for even a fairly easy soup, it is then even more difficult when there are dozens of steps and different pathsways

```{r, fig.width=8}
plot(g,layout= layout.sugiyama(g)$layout)
```

***

Furthermore a clean workflow allows you to better parallelize the task since it is clear which tasks can be performed independently
```{r, fig.width=8}
V(g)[c(1,3,7,8)]$color <- "red"
V(g)[c(4)]$color <- "green"
V(g)[c(2,9,10)]$color <- "yellow"
plot(g,layout= layout.sugiyama(g)$layout)
```


