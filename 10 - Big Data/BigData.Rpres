```{r global_setup,  warning=FALSE, cache=FALSE,echo=FALSE,error=FALSE,results='hide'}
require(knitr)
# default settings, # settings for presentation version
echo.val<-F
fig.height<-5
dpi<-150
cache<-T
fig.path<-"pres_figures/"
cache.path<-"pres_cache/"

if(exists("printed")) { # settings for printed version (if the variable exists)
  echo.val<-T # show code
  fig.height<-3
  dpi<-150
  cache<-T
  fig.path<-"print_figures/"
  cache.path<-"print_cache/"
}
opts_chunk$set(dpi=dpi,cache=cache,
               cache.path=cache.path,results='hide',
               warning=F,fig.align='center',echo=echo.val,
               fig.height=fig.height,fig.path=fig.path,message=F) #dev="CairoPNG"
```

```{r script_setup,results='hide',cache=FALSE}
require(ggplot2)
require(plyr)
require(grid) # contains the arrow function
require(biOps)
require(doMC) # for parallel code
require(EBImage)
## To install EBImage
# source("http://bioconductor.org/biocLite.R")
# biocLite("EBImage")

# start parallel environment
registerDoMC()
# functions for converting images back and forth
im.to.df<-function(in.img) {
    out.im<-expand.grid(x=1:nrow(in.img),y=1:ncol(in.img))
    out.im$val<-as.vector(in.img)
    out.im
}
df.to.im<-function(in.df,val.col="val",inv=F) {
  in.vals<-in.df[[val.col]]
  if(class(in.vals[1])=="logical") in.vals<-as.integer(in.vals*255)
  if(inv) in.vals<-255-in.vals
  out.mat<-matrix(in.vals,nrow=length(unique(in.df$x)),byrow=F)
  attr(out.mat,"type")<-"grey"
  out.mat
}
ddply.cutcols<-function(...,cols=1) {
  # run standard ddply command
  cur.table<-ddply(...)
  cutlabel.fixer<-function(oVal) {
    sapply(oVal,function(x) {
      cnv<-as.character(x)
      mean(as.numeric(strsplit(substr(cnv,2,nchar(cnv)-1),",")[[1]]))
    })
  }
  cutname.fixer<-function(c.str) {
    s.str<-strsplit(c.str,"(",fixed=T)[[1]]
    t.str<-strsplit(paste(s.str[c(2:length(s.str))],collapse="("),",")[[1]]
    paste(t.str[c(1:length(t.str)-1)],collapse=",")
  }
  for(i in c(1:cols)) {
    cur.table[,i]<-cutlabel.fixer(cur.table[,i])
    names(cur.table)[i]<-cutname.fixer(names(cur.table)[i])
  }
  cur.table
}
```

Scaling Up
========================================================
author: Kevin Mader
date: 25 March 2014
width: 1440
height: 900
transition: rotate
css: ../template.css
## TOMCAT Progress Report


Outline
========================================================
- Motivation
- Key Projects
- Parallel and Distributed Computing
- Scaling Imaging
- Scaling Post-processing
- Beyond


Motivation
===
There are three different types of problems that we will run into.
### Really big data sets
- Several copies of the dataset need to be in memory for processing
- Computers with more 256GB are expensive and difficult to find
- Even they have 16 cores so still 16GB per CPU
- Drive speed / network file access becomes a limiting factor
- If it crashes you __lose__ everything
 - or you have to manually write a bunch of mess check-pointing code


***

### Many datasets
- For genome-scale studies 1000s of samples need to be analyzed identically
- Dynamic experiments can have hundreds of measurements 
- Animal phenotyping can have many huge data-sets (1000s of 328GB datasets)

### Exploratory Studies
- Not sure what we are looking for
- Easy to develop new analyses
- Quick to test hypotheses

Key Projects
===

### Zebra fish Phenotyping Project
<small>Collaboration with Keith Cheng, Ian Foster, Xuying Xin, Patrick La Raviere, Steve Wang</small>

1000s of samples of full adult animals, imaged at 0.74 $\mu m$ resolution: Images
11500 x 2800 x 628 $\longrightarrow$ 20-40GVx / sample

<video controls>
  <source src="ext-figures/fish-scanthrough.mov" type="video/mp4">
Your browser does not support the video tag.
</video>

- Identification of single cells (no down-sampling)
- Cell networks and connectivity
- Classification of cell type
- Registration with histology

***

### Brain Project
<small>Collaboration with Alberto Astolfo, Matthias Schneider, Bruno Weber, Marco Stampanoni</small>

1 $cm^3$  scanned at 1 $\mu m$ resolution: Images $\longrightarrow$ 1000 GVx / sample

- Registration separate scans together
- Blood vessel structure and networks
- Registration with fMRI, histology

Definition: Big Data
===
### Velocity, Volume, Variety
When a ton of heterogeneous is coming in fast.

__Performant, scalable, and flexible__

### When scaling isn't scary
10X, 100X, 1000X is the same amount of effort

### When you are starving for enough data
Director of AMPLab said their rate limiting factor is always enough interesting data

### O 'clicks' per sample

But Kevin you've already done big data!
===
incremental: true
- Umm, __no__.
- My samples were sized 1024 x 1024 x 1024 $\approx$ 1 GVx.
- They had two nice phases that were always the same (homogeneous)
- They were analyzed on single computers using 4-6 cores and took 12-24 hours for the full analysis.
- Zebra Fish Project
 - need computers with at least 20X as much memory (__400GB__)
 - it would take __240 hours__ per sample...
 - best case scenario GWS-3 consumed 100% for 3 days
- Brain Project
 - need computers with 1000X as much memory (__20TB.__)
 - it would take __16 months__ per sample...
 
But what did you do then?
===

- TIPL is a flexible (if not __so__ scalable) framework for image processing and testing
 - modular design
 - verification and testing
 - reproduction of results from parameters 
 - cluster integration

- Report Generation
 - parametric assessment
 - pipelined analysis
 - Rendering subregions for movies
 - Displaying multidimensional data efficiently
 
***

![TIPL](ext-figures/TIPLoutline.png)

What is parallelism?
===

Parallelism is when you can divide a task into separate pieces. Some tasks are easy to parallelize while others are very difficult. Rather than focusing on programming, real-life examples are good indicators of difficultly.

1. You have 10 friends who collectively know all the capital cities of the world.
 - To find the capital of a single country you just yell the country and wait for someone to respond (+++)
 - To find who knows the most countries, each, in turn, yells out how many countries they know and you select the highest (++)
 
***

1. Each friend has some money with them
 - To find the total amount of money you tell each person to tell you how much money they have and you add it together (+)
 - To find the __median__ coin value, you ask each friend to tell you you all the coins they have and you make one master list and then find the median coin (-)

What is distributed computing?
===
Distributed computing is very similar to parallel computing, but a bit more particular. Parallel means you process many tasks at the same time, while distributed means you are no longer on the same CPU, process, or even on the same machine.

The distributed has some important implications since once you are no longer on the same machine the number of variables like network delay, file system issues, and other users becomes a major problem.

Approaches
===

### Imperative 
- Languages like C, C++, Java, Matlab
- Exact orders are given
- Data management is manually controlled
- Job and task scheduling is manual
- Potential to tweak and optimize performance


***

### Declarative 
- Languages like SQL, Erlang, Haskell, Scala, Python, R
- Goals are stated rather than specific details
- Data is automatically managed and copied
- Scheduling is automatic but slow




Parallel Problems
===

### Coordination
Parallel computing requires a significant of coordinating between computers for non-easily parallelizable tasks.

### Mutability
The second major issue is mutability, if you have two cores / computers trying to write the same information at the same it is no longer deterministic (not good)

### Blocking
The simple act of taking turns and waiting for every independent process to take its turn can completely negate the benefits of parallel computing

Distributed Problems
===

Inherits all of the problems of parallel programming with a whole variety of new issues.

### Sending Instructions / Data Afar

### Fault Tolerance
If you have 1000 computers working on solving a problem and one fails, you do not want your whole job to crash

### Data Storage

How can you access and process data from many different computers quickly without very expensive infrastructure


General Problems
===

### Implementation
Each parallel library requires different tools with different constraints CUDA, OpenCL, OpenMPI, Matlab Distributed Toolbox, KORUS, .NET Concurrent Framework, Python Multiprocessing, Hadoop, Storm, Impala, Redshift

- Learn many very specific code bases, and __managing__ the logistics of organizing.
- Almost none are fault-tolerant out of the box.
- _Lock in_: Transitioning from one code-base to another is at best nightmarish
- Testing and profiling code locally can be very difficult
- None of them are flexible allowing extra-processing power to "jump" in intense complications when it is needed

The Solution: Spark / Resilient Distributed Datasets
===

### Technical Specifications

- Developed by the Algorithms, Machines, and People Lab at UC Berkeley in 2012
- General tool for all Directed Acyclical Graph (DAG) workflows
- Course-grained processing $\rightarrow$ simple operations applied to entire sets
 - Map, reduce, join, group by, fold, foreach, filter,...
- In-memory caching

<small>Zaharia, M., et. al (2012). Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing</small>
***

### Practical Specification
- Distributed, parallel computing without __logistics__, libraries, or compiling
- Declarative rather than imperative
 - Apply operation $f$ to each image / block
 - __NOT__ tell computer 3 to wait for an image from computer 2 to and perform operation $f$ and send it to computer 1
 - Even scheduling is handled automatically
- Results can be stored in memory, on disk, redundant or not

Hadoop / MapReduce
===

Hadoop is the underlying infrastructure and filesystem that handles storing and distributing data so each machine stores some of the data locally and processing jobs run where the data is stored. 
- No expensive Infini-Band required.
 - Non-local data is copied over the network. 
- Storage is automatically expanded with processing power.
- It's how Google, Amazon, Microsoft, Yahoo, Facebook, ... deal with exabytes of data

Starting Spark
===
[Spin up your own cluster in an hour](https://amplab.cs.berkeley.edu/2013/10/23/got-a-minute-spin-up-a-spark-cluster-on-your-laptop-with-docker/)

1. Start a master node ```start-master.sh``` 
1. Start several worker nodes 

```start-worker.sh spark://master-ip:7077 -c #CORES -m #MEM```

1. Start the Spark-Shell ```spark-shell.sh```
1. Write code in Scala, Java, Python, or R

Perform a threshold
===

```
val threshVal=127
val labelImg=inImg.filter(_._2>threshVal)
```

- Runs on 1 core on your laptop or 1000 cores in the cloud or on Merlin or the beamline.
- If one computer crashes or disconnects it __automatically__ continues on another one. 
- If one part of the computation is taking too long it will be sent to other computers to finish
- If a computer runs out of memory it writes the remaining results to disk and continues running (graceful dropoff in performance )

Region of Interest
===
Take a region of interest between 0 and 100 in X,Y, and Z
```{java}
def roiFunc(pvec: ((Int,Int,Int),Double)) = 
 {pvec._1._1>=0 & pvec._1._1<100 & // X
  pvec._1._2>=0 & pvec._1._2<100 & //Y
  pvec._1._3>=0 & pvec._1._3<100} // Z
  
rImg.filter(roiFunc)
```

Perform a 3x3x3 box filter
===
```
def spread_voxels(pvec: ((Int,Int,Int),Double), windSize: Int = 1) = {
  val wind=(-windSize to windSize)
  val pos=pvec._1
  val scalevalue=pvec._2/(wind.length**3)
  for(x<-wind; y<-wind; z<-wind) 
    yield ((pos._1+x,pos._2+y,pos._3+z),scalevalue)
}

val filtImg=roiImg.
      flatMap(cvec => spread_voxels(cvec)).
      filter(roiFun).reduceByKey(_ + _)
```

Perform component labeling
===
```{, size="tiny"}
var groupList=Array((0L,0))
var running=true
var iterations=0
while (running) {
  val newLabels=labelImg.
  flatMap(spread_voxels(_,1)).
    reduceByKey((a,b) => (math.min(a._1,b._1),(a._2 | b._2))).
    filter(_._2._2). // keep only voxels which contain original pixels
    map(pvec => (pvec._1,pvec._2._1))
  // make a list of each label and how many voxels are in it
  val curGroupList=newLabels.map(pvec => (pvec._2,1)).
    reduceByKey(_ + _).sortByKey(true).collect
  // if the list isn't the same as before, continue running since we need to wait for swaps to stop
  running = (curGroupList.deep!=groupList.deep)
  groupList=curGroupList
  labelImg=newLabels
  iterations+=1
  print("Iter #"+iterations+":"+groupList.mkString(","))
}
groupList
```

Lazy evaluation
===
```
val outThresh=inImg.map(threshFunc)
val outLabels=runCL(outThresh)
outLabels.filter(roiFunc).saveImage('test.tif')
```

- No execution starts until you save the file or require output
- Spark automatically deconstructs the pipeline and optimizes the jobs to run so computation is not wasted outside of the region of interest (even though we did it last)


Real Spark Performance
===
```{r, fig.cap="Real Performance",fig.height=6}
map.data.dir<-"/Users/mader/Dropbox/WorkRelated/Papers/MapReduce3D/ext-data"
pmd<-function(...,sep="") paste(map.data.dir,...,sep="/")
 read.fcn<-function(filename) {
  in.table<-read.csv(filename,header=F)
  col.names<-c("Type","Master","iters","x","y","z","MapTime","RunTime")
  if(ncol(in.table)>10) col.names<-c(col.names,"mapsteps","nodes","persist","compress")
  names(in.table)<-col.names
  ext.nodes<-function(x) as.numeric(substr(x,7,nchar(x)-1))
  if(ncol(in.table)<10) {
    in.table$nodes<-sapply(as.character(in.table$Master),ext.nodes)
    in.table$persist<-0
    in.table$compress<-F
  }
  in.table$storage<-sapply(in.table$persist,function(x) 
    switch(x+1,"Memory","Memory (S)","Memory-Disk","Memory-Disk (S)","Only Disk")
  )
  in.table$compress<-as.logical(in.table$compress)
  in.table$compression<-ifelse(in.table$compress,"compressed","uncompressed")
  fc<-function(x,width=4,...) formatC(x,width=width,...)
  in.table$dim<-with(in.table,paste(fc(x),fc(y),fc(z),sep=","))
  in.table
}
fit.data<-rbind(cbind(read.fcn(pmd("dist_filter.csv")),source="Distributed"),
                cbind(read.fcn(pmd("merlinl02_filter.csv")),source="Single",mapsteps=-1))

fit.data$voxpersec<-with(fit.data,iters*as.numeric(x)*y*z/(RunTime/1000))
ggplot(fit.data,aes(x=nodes,y=voxpersec
                    )
       )+
  geom_point(aes(shape=source,
                 color=as.factor(paste(formatC(round(as.numeric(x)*y*z/1e8)/10,width=6,format="f",digits=1)," GVx"))))+
  geom_smooth()+
  labs(title="Filter Performance",x="Cores",y="Voxels per Second",color="Data Size",shape="Operation Mode")+
  theme_bw(20)
```

***

```{r, fig.cap="Overhead Costs",fig.height=6}
table.ready<-ddply.cutcols(fit.data,.(x,cut_interval(nodes,6)),function(c.blocks) c.blocks[1,])
ggplot(subset(table.ready,source=="Distributed"),
       aes(y=formatC(round(as.numeric(x)*y*z),format="e",digits=3),x=as.factor(round(nodes))))+
  geom_tile(aes(fill=MapTime/(RunTime*nodes)*100))+
  geom_text(aes(label=paste(round(MapTime/(RunTime*nodes)*100),"%",sep="")),color="red")+
  theme_bw(20)+labs(y="Voxels",x="Cores",fill="Time spent\n in Map (%)",title="Utilization / Overhead Estimates")
```

TOMCAT Imaging Goals
===

### Needs
- Scale up to 1000GVx samples (eventually)
- Analyze standard measurements 14GVx regularly in a day
- Analyze as fast as we usually measure 15 minutes / scan

### Would be nice

- Analyze scans as fast as we measure now (1 scan / minute)
- Analyze scans as fast as we could measure with Gigafrost (10 scans / minute)

***

```{r, fig.cap="Tomcat Goals",fig.height=4}
distfilter.csv<-read.fcn(pmd("dist_filter.csv"))
fit.data<-distfilter.csv
fit.data$voxpersec<-with(fit.data,iters*as.numeric(x)*y*z/(RunTime/1000))

filter.lm<-lm(voxpersec~(nodes),data=fit.data)
predict.table<-data.frame(name=c("Kevin's Laptop","GWS-3","Beamline Standard","Beamline Full","Merlin Full","CSCS Monte Rosa"),nodes=c(2,20,60,12*6+20,30*12+16,47872),exp="Filter")
predict.table$est.voxpersec<-predict(filter.lm,predict.table)

goal.table<-data.frame(name=c("1","10\n Gigafrost","0.001\n15 min/scan\n","0.0002\n20 scans/day\n"),
                       gvph=(2560*2560*2160)/1e9*c(3600,10*3600,4,20/24),
                       x.start=min(predict.table$nodes),x.end=max(predict.table$nodes))
ggplot(predict.table,aes(x=nodes,y=est.voxpersec*3600/1e9)
       )+
  geom_segment(data=goal.table,aes(color=name,x=x.start,xend=x.end,y=gvph,yend=gvph),alpha=0.8,size=2)+
  geom_line(color="black")+
  geom_point(aes(shape=name),size=5)+
  
  labs(x="Nodes Used",y="Gigavoxels per hour",shape="Cluster",color="Goal\nOps/s",title="Log-Log")+
  scale_y_log10()+scale_x_log10()+theme_bw(15)
```

```{r, fig.cap="Tomcat Goals",fig.height=4}
ggplot(predict.table,aes(x=nodes,y=est.voxpersec*3600/1e9)
       )+
  geom_segment(data=goal.table,aes(color=name,x=0,xend=376,y=gvph,yend=gvph),alpha=0.8,size=2)+
  geom_line(color="black")+
  geom_point(aes(shape=name),size=5)+
  labs(x="Nodes Used",y="Gigavoxels per hour",shape="Cluster",color="Goal\nOps/s",title="Linear Scale")+
  xlim(0,400)+ylim(0,200)+theme_bw(15)
```


Post-processing: Statistical Analysis
===

- Exploring phenotypes of 55 million cells. 
- Our current approach has been either
- Group and divide then average the results.
 - Average Cell Volume, etc.
 - Average density or connectivity
- Detailed analysis of individual samples
 - K-means clustering for different regions in bone / layers
 - Principal component analysis for phenotypes
 
***

If we do that, we miss a lot!
```{r load_data}
data.dir<-"/Users/mader/Dropbox/WorkRelated/Papers/iwbbio2014"
pd<-function(...,sep="") paste(data.dir,...,sep="/")
# read and correct the coordinate system
# Routine for Connecting to the Database
raw.summary<-read.csv(pd("summary.csv"))
# fix column names
raw.names<-names(raw.summary)
names(raw.summary)<-sapply(raw.names,function(x) gsub("[.]1","_SD",x))
raw.summary<-subset(raw.summary,VOLUME>0 & VOLUME<1000 & count<100000 & count>10000) # filter out the corrupt files
```

```{r, results='asis'}
mean.nona<-function(...) mean(...,na.rm=T)
sd.nona<-function(...) sd(...,na.rm=T)
imp.phenotypes<-names(raw.summary)[c(9:11,15,19,23,24:30,35,38,42)]
imp.phenotypes<-names(raw.summary)[c(15,19,23,29,35,38,42,44,45)]
var.pheno<-function(in.df,in.col) {
  sd.col<-paste(in.col,"_SD",sep="")
  mean.vals<-in.df[,in.col]
  sd.vals<-in.df[,sd.col]
  ie.var<-c(sd.nona(mean.vals)/mean.nona(abs(mean.vals)))*100
  ia.var<-c(mean.nona(sd.vals)/mean.nona(abs(mean.vals)))*100
  data.frame(Phenotype=c(in.col),
             Within.Sample.CV=ia.var,
             Between.Sample.CV=ie.var,
             Ratio=ia.var/ie.var*100)   
}
out.table<-ldply(imp.phenotypes,function(col.name) var.pheno(raw.summary,col.name))
names(out.table)<-c("Phenotype","Within","Between","Ratio (%)")
out.table$Phenotype<-c("Length","Width","Height","Volume","Nearest Canal Distance","Density (Lc.Te.V)","Nearest Neighbors (Lc.DN)","Stretch (Lc.St)","Oblateness (Lc.Ob)")
kable(out.table)
```
<small>The results in the table show the within and between sample variation for selected phenotypes in the first two columns and the ratio of the within and between sample numbers</small>

Visualizing the Variation
===
How does this result look visually? Each line shows the mean $\pm$ standard deviation for sample. The range within a single sample is clearly much larger than between
```{r, fig.cap="Partial Volume Effect",fig.height=4}
ggplot(raw.summary,aes(x=as.numeric(as.factor(VOLUME)),y=VOLUME,ymin=1e9*VOLUME-1e9*VOLUME_SD,ymax=1e9*VOLUME+1e9*VOLUME_SD))+geom_errorbar()+geom_point()+theme_bw(25)+labs(x="Sample",y=expression(paste("Volume (",um^3,")")))
```

Do not condense
===
With the ability to scale to millions of samples there is no need to condense. We can analyze the entire dataset in real-time and try and identify groups or trends within the whole data instead of just precalculated views of it.

### Practical
1276 comma separated text files with 56 columns of data and 15-60K rows

```{r, results='asis'}
results.df<-data.frame(Task=c("Load and Preprocess","Single Column Average","1 K-means Iteration"),
                       Single.Time=c("360 minutes","4.6s","2 minutes"),
                       Spark.Time=c("10 minutes","400ms","1s"))
names(results.df)[2]<-"Single Core Time"
names(results.df)[3]<-"Spark Time (40 cores)"
kable(results.df)
```

### Can iteratively explore and hypothesis test with data quickly

We found several composite phenotypes which are more consistent within samples than between samples

Rich, heavily developed platform
===

### Available Tools
Tools built for table-like data data structures and much better adapted to it. 
 - [K-Means](https://github.com/apache/incubator-spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala), [Correlation](https://github.com/freeman-lab/thunder/blob/master/python/thunder/sigprocessing/localcorr.py), [PCA](https://github.com/freeman-lab/thunder/blob/master/python/thunder/factorization/pca.py)
 - [Matrix Factorization](https://amplab.cs.berkeley.edu/projects/dfc-%C2%A0divide-and-conquer-matrix-factorization/), [Genomics](https://amplab.cs.berkeley.edu/projects/dna-processing-pipeline/), [Graph Analytics](https://amplab.cs.berkeley.edu/projects/graphx/), [Machine Learning](https://amplab.cs.berkeley.edu/projects/mlbase/)

### Commercial Support
Dozens of major companies (Apple, Google, Facebook, Cisco, ...) donate over $30M a year to development of Spark and the Berkeley Data Analytics Stack 
- 2 startups in the last 6 months with seed-funding in excess of $15M each

### Academic Support
- All source code is available on GitHub
 - Elegant (20,000 lines vs my PhD of 75,000+)
- No patents or restrictions on usage
- Machine Learning Course in D-INFK next semester based on Spark


Beyond: Streaming
===

### Post-processing goals
- Analysis done in weeks instead of months
- Some real-time analysis and statistics

### Streaming
Can handle static data or live data coming in from a 'streaming' device like a camera to do real-time analysis.
The exact same code can be used for real-time analysis and static code

### Scalability

#### Connect more computers. 
#### Start workers on these computer (~~Full Stop~~).

***

Source could the camera server itself or a watched directory on a machine.

```{r , fig.height=7}
library(igraph)
node.names<-c("Projection\nImages","Most Recent\n1500 images","ROI\nTracking","Exposure\nAdaption",
              "Flat-field\nCorrection","Sinogram\nGeneration","GridRec","Filtering",
              "Segmentation","Histogram","Center\nEstimation","Shape\nAnalysis")
c.mat<-matrix(0,length(node.names),length(node.names))
colnames(c.mat)<-node.names
rownames(c.mat)<-node.names
c.mat["Projection\nImages","Flat-field\nCorrection"]<-1
c.mat["Projection\nImages","Exposure\nAdaption"]<-1
c.mat["Flat-field\nCorrection","Most Recent\n1500 images"]<-1
c.mat["Most Recent\n1500 images","Sinogram\nGeneration"]<-1
c.mat["Flat-field\nCorrection","ROI\nTracking"]<-1
c.mat["Sinogram\nGeneration","GridRec"]<-1
c.mat["Sinogram\nGeneration","Center\nEstimation"]<-1
c.mat["Center\nEstimation","GridRec"]<-1
c.mat["GridRec","Filtering"]<-1
c.mat["Filtering","Histogram"]<-1
c.mat["Filtering","Segmentation"]<-1
c.mat["Segmentation","Shape\nAnalysis"]<-1
g<-graph.adjacency(c.mat,mode="directed")
V(g)$degree <- degree(g)
V(g)$label <- V(g)$name
V(g)$color <- "lightblue"
V(g)["Projection\nImages"]$color<-"red"
V(g)["Flat-field\nCorrection"]$color<-"green"
V(g)["Exposure\nAdaption"]$color<-"green"
V(g)["Sinogram\nGeneration"]$color<-"green"
V(g)["ROI\nTracking"]$color<-"green"
V(g)["Center\nEstimation"]$color<-"green"
V(g)["GridRec"]$color<-"green"
V(g)["Most Recent\n1500 images"]$color<-"green"
V(g)["Most Recent\n1500 images"]$frame.width<-2
V(g)$size<-35
E(g)$width<-2
E(g)$color<-"black"
E(g)[8]$width<-5
E(g)[8]$color<-"red"
plot(g,  layout=layout.fruchterman.reingold)# layout.kamada.kawai) #layout=layout.circle)#
```

Beyond: Approximate Results
===

Projects at AMPLab like Spark and BlinkDB are moving towards approximate results.
- Instead of ```mean(volume)```
 - ```mean(volume).within_time(5)```
 - ```mean(volume).within_ci(0.95)```

For real-time image processing it might be the only feasible solution and could drastically reduce the amount of time spent on analysis.



Beyond: Image SQL
===
Structure Query Language (SQL) forms the basis for almost all database languages that exist today.
- Standard language that allows the computer and cluster to query as it sees fit
- Used for Big (Hive, Impala) and Small (SQLite) data-alike

```{sql}
SELECT customer_name, sum(price_paid) FROM sales_db GROUP BY customer_name
```

***

Image SQL would be an image processing domain specific language (DSL) allowing image processing to be written as simple commands and computers / clusters divide and run the jobs

```
SELECT mean(VOLUME) FROM GAUSSIAN_FILTER(input_image) GROUP BY COMPONENT_LABEL
```

Spark Preview
===

<iframe src='ext-figures/FilterTest - Spark Stages.html' width='100%' height='800'></iframe>


Acknowledgements
===
- AIT at PSI
- TOMCAT Group
![Tomcat Group](ext-figures/tgp.png)


Principles
===
### Disclosure : There are entire courses / PhD thesis's / Companies about this, so this is just a scant introduction
- Parallel and Distributed Computing
- Threads
- Shared-Memory
- Race Conditions
- Synchronization
 - [Dead lock](http://en.wikipedia.org/wiki/Dining_philosophers_problem)
 
 
 Course Outline
========================================================
- 20th February - Introductory Lecture
- 27th February - Filtering and Image Enhancement (A. Kaestner)
- 6th March - Basic Segmentation, Discrete Binary Structures
- 13th March - Advanced Segmentation
- 20th March - Analyzing Single Objects
- 27th March -  Analyzing Complex Objects
- 3rd April -  Spatial Distribution
- 10th April -  Statistics and Reproducibility
- 17th April - Dynamic Experiments
- 8th May - **Big Data**
- 15th May - Guest Lecture - Applications in Material Science
- 22th May - Project Presentations

Literature / Useful References
========================================================
- [Google's Presentation on Distributed Computing](http://www.youtube.com/watch?v=yjPBkvYh-ss&feature=youtu.be)
 - [Slides](http://www.slideshare.net/tugrulh/google-cluster-computing-and-mapreduce-introduction-to-distributed-system-design)
- [Scalable Systems Course](https://courses.cs.washington.edu/courses/cse490h/08au/)
- [Tutorial in Hadoop](http://www.youtube.com/watch?v=KwW7bQRykHI)
- [Intro to Data Science @UCB](http://amplab.github.io/datascience-sp14/)
