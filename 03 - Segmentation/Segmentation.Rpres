```{r global_setup, echo=FALSE, warning=FALSE, cache=FALSE}
opts_chunk$set(dev="CairoPNG",dpi=300)
library(ggplot2)
```

Quantitative Big Imaging  Segmentation
========================================================
author: Kevin Mader
date: 6 March 2014
width: 1440
height: 900
transition: rotate

Course Outline
========================================================
- 20th February - Introductory Lecture
- 27th February - Filtering and Image Enhancement (A. Kaestner)
- 6th March - **Basic Segmentation, Discrete Binary Structures**
- 13th March - Advanced Segmentation
- 20th March - Analyzing Single Objects
- 27th March -  Analyzing Complex Objects
- 3rd April -  Spatial Distribution
- 10th April -  Statistics and Reproducibility
- 17th April - Dynamic Experiments
- 8th May - Big Data
- 15th May - Guest Lecture - Applications in Material Science
- 22th May - Project Presentations

Literature
========================================================
- Jean Claude, Morphometry with R,(Online through ETHZ at http://link.springer.com/book/10.1007%2F978-0-387-77789-4  Buy it at: http://www.amazon.com/Morphometrics-R-Use-Julien-Claude/dp/038777789X
- John C. Russ, “The Image Processing Handbook”,(Boca Raton, CRC Press(Available online within domain ethz.ch (or proxy.ethz.ch / public VPN) http://dx.doi.org/10.1201/9780203881095

Motivation:  Why do we do imaging experiments?
========================================================
- To get an idea of what is going on
- To test a hypothesis
 - Does temperature affect bubble size?
 - Is this gene important for cell shape and thus mechanosensation in bone?
 - Does higher canal volume make bones weaker?
 - Does the granule shape affect battery life expectancy?


To test a hypothesis
========================================================
- We perform an experiment bone to see how big the cells are inside the tissue
$$\downarrow$$ ![Bone Measurement](ext-figures/tomoimage.png) 

### 2560 x 2560 x 2160 x 32 bit = 56GB / sample
- Filtering and Preprocessing!  
$$\downarrow$$
- 20h of computer time later ...
- 56GB of less noisy data
- Way too much data, we need to reduce

What did we want in the first place
========================================================
### _Single number_:
* volume fraction,
* cell count,
* average cell stretch,
* cell volume variability

Where does segmentation get us?
========================================================
type: sub-section
incremental: true

- We convert a decimal value (or something even more complicated like 3 values for RGB images, a spectrum for hyperspectral imaging, or a vector / tensor in a mechanical stress field)
- Each point reduces to a single value  

### 2560 x 2560 x 2160 x 32 bit = 56GB / sample
$$\downarrow$$
### 2560 x 2560 x 2160 x **1 bit** = 1.75GB / sample

Applying a threshold to an image
========================================================
```{r, echo=FALSE,fig.cap=""}
grad.im<-expand.grid(x=c(1:5),y=c(1:5))
grad.im<-cbind(grad.im,col=runif(nrow(grad.im)))
ggplot(grad.im,aes(x=x,y=y,fill=col))+
  geom_tile()+
  scale_fill_gradient(low="black",high="white")+
  labs(fill="Intensity")+
  theme_bw(25)
```
***
```{r, echo=FALSE,fig.cap="With Threshold Overlay"}
ggplot(grad.im,aes(x=x,y=y))+
  geom_tile(aes(fill=col))+
  geom_tile(data=subset(grad.im,col>0.5),fill="red",color="black",alpha=0.3)+
  geom_tile(data=subset(grad.im,col<0.5),fill="blue",color="black",alpha=0.3)+
  scale_fill_gradient(low="black",high="white")+
  labs(fill="Intensity")+
  theme_bw(25)
```
***
Hello









